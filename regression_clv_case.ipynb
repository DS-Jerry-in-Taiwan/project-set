{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regression clv case.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DS-Jerry-in-Taiwan/project-set/blob/main/regression_clv_case.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd3FdfgRfDhr"
      },
      "source": [
        "# Customer Lifetime Value and Boston House Price"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNkFLcDQmugY"
      },
      "source": [
        "- <font size=4>[回歸實戰](https://www.xuemi.co/programs/8d6e9c07-87c7-4b34-99c7-2616d9bcc605/contents?back=project_50b678a6-14b0-4148-b706-724d7e834e20]</font>\n",
        "- <font size=3>https://reurl.cc/8Gagoy</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkX-uqMQggLa"
      },
      "source": [
        "# 載入程式庫及必要定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZaLIy7QfBIU"
      },
      "source": [
        "try:\n",
        "    from google.colab import drive, files\n",
        "    in_colab = True\n",
        "except ModuleNotFoundError:\n",
        "    in_colab = False\n",
        "\n",
        "if in_colab:\n",
        "    home_dir = ''\n",
        "    drive.mount('/content/drive')\n",
        "    groot_dir = '/content/drive/MyDrive/adventure_time/'\n",
        "else:\n",
        "    from pathlib import Path\n",
        "    home_dir = str(Path.home())\n",
        "    groot_dir = home_dir + '/Google Drive/adventure_time/'\n",
        "\n",
        "import matplotlib as mpl\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=14)\n",
        "mpl.rc('ytick', labelsize=14)\n",
        "mpl.rc('font', size=14)\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import *\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import gdown\n",
        "import requests\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "\n",
        "figure_dir = groot_dir + 'figure/regression/'\n",
        "data_dir = groot_dir + 'regression/'\n",
        "\n",
        "gfigure = lambda name: figure_dir + name + '.png'\n",
        "output_fig = lambda name: plt.savefig( gfigure(name), dpi = 300)\n",
        "\n",
        "local_time = lambda x, offset: x + relativedelta(hours= offset)\n",
        "def local_now(hours = 8):\n",
        "    return datetime.now() + relativedelta(hours = hours if in_colab else 0)\n",
        "\n",
        "def print_now():\n",
        "    return print(local_now())\n",
        "\n",
        "def print_local_now():\n",
        "    return print('Local Time:', local_now())\n",
        "\n",
        "def DropboxLink(did, fname):\n",
        "    return 'https://dl.dropboxusercontent.com/s/%s/%s' % \\\n",
        "    (did, fname)\n",
        "\n",
        "def fetch_gdrive_file(fid, local_save):\n",
        "    remote_url = 'https://drive.google.com/uc?id=' + fid\n",
        "    gdown.download(remote_url, local_save, quiet = False)\n",
        "\n",
        "def fetch_file_via_requests(url, save_in_dir):\n",
        "    local_filename = url.split('/')[-1]\n",
        "    # NOTE the stream=True parameter below\n",
        "    output_fpath = save_in_dir + local_filename\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(output_fpath, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192): \n",
        "                if chunk: # filter out keep-alive new chunks\n",
        "                    f.write(chunk)\n",
        "                    # f.flush()\n",
        "    return output_fpath\n",
        "        \n",
        "def acct_string(num):\n",
        "    s0 = str(num)\n",
        "    if len(s0) <=3:\n",
        "        return s0  \n",
        "    num_section = int(len(s0)/3)\n",
        "    remaining_start = len(s0) % 3\n",
        "    s = s0[:remaining_start]\n",
        "    for i in range(num_section):\n",
        "        s += ',%s' % s0[remaining_start + i*3 :remaining_start + (i+1)*3]   \n",
        "    return s\n",
        "\n",
        "def round_up(n, decimals=0):\n",
        "    multiplier = 10 ** decimals\n",
        "    return math.ceil(n * multiplier) / multiplier\n",
        "\n",
        "def round_down(n, decimals=0):\n",
        "    multiplier = 10 ** decimals\n",
        "    return math.floor(n * multiplier) / multiplier\n",
        "\n",
        "def start_plot(figsize=(10, 8), style = 'whitegrid'):\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    gs = fig.add_gridspec(1,1)\n",
        "    plt.tight_layout()\n",
        "    with sns.axes_style(style):\n",
        "        ax = fig.add_subplot(gs[0,0])\n",
        "    return ax\n",
        "\n",
        "EX1DATA = '147xBeCECYur0FxDyly-oG2BqsqEH2Mxm'\n",
        "EX1DATA2 = '101qw-9OkjCxwuSkBJUBaGURRWpZbFKOe'\n",
        "EX5DATA = '1nNM8CN9CkRfjipRx1qJYZhSobmABVL1J'\n",
        "ADVER = '1xFMcCuiMgX9VnelDtbyyV9rXBMFerx8k'\n",
        "TAIWAN_CSV = '1I5yqulrZSHPSQkxT3oqt_3uVAhPolOEP'\n",
        "JHU_CSSE = 'https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/'\n",
        "MNIST_TRAIN = '1E-uJ0zqqAfpsVjoOSzqF5TXhDfPNlkQ5'\n",
        "MNIST_TRAIN_LABEL = '13clNJ2cd2I90W3DEkDBKjZSDNNEqqx3B'\n",
        "MNIST_TEST = '1zVpVHJl5YABa3qExt1K-O3WaEHXTJekg'\n",
        "MNIST_TEST_LABEL = '1qci_-dqubnRN-cdrCsbYaUAxyO7_jH9z'\n",
        "\n",
        "print('\\nRunning on %s' % sys.platform)\n",
        "print('Python Version', sys.version)\n",
        "print('Data storage points to ==>', data_dir)\n",
        "\n",
        "print('\\nThis module is amied to leran regression basics...') \n",
        "print('\\nLibraries and dependenciess imported')\n",
        "print_now()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qN8uR68q3L_"
      },
      "source": [
        "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX9cRKDSgnco"
      },
      "source": [
        "# 下載資料檔案"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnHtB4KnfPyj"
      },
      "source": [
        "fetch_file_via_requests(\n",
        "    DropboxLink('e7lsf1k6258w7co', 'ec_201012_test_4.csv'), data_dir )\n",
        "\n",
        "fetch_file_via_requests(\n",
        "    DropboxLink('fwokyefy0looizp', 'ec_201012_train_4.csv'), data_dir )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ3g87efqvap"
      },
      "source": [
        "#檢視sklearn.metrics的score 可以有哪些選擇\n",
        "import sklearn.metrics\n",
        "for x in sklearn.metrics.SCORERS.keys():\n",
        "    print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO7SN2TGg1SP"
      },
      "source": [
        "# Preparing Data\n",
        "\n",
        "訓練資料與測試（test）資料的名稱 ec_201012_train_4.csv 和 ec_201012_test_4.csv, 這兩個資料集的欄位相同，包括：\n",
        "\n",
        "```\n",
        "'CustomerID', 'date_size', 'date_recency', 'date_time_between',\n",
        "'date_T', 'baseket_value_sum', 'baseket_value_mean',\n",
        "'baseket_value_std', 'baseket_value_amax', 'baseket_value_amin',\n",
        "'baseket_value_median', 'basket_size_sum', 'basket_size_mean',\n",
        "'basket_size_std', 'basket_size_amax', 'basket_size_amin',\n",
        "'basket_size_median', 'lag_12', 'lag_11', 'lag_10', 'lag_9', 'lag_8',\n",
        "'lag_7', 'lag_6', 'lag_5', 'lag_4', 'lag_3', 'lag_2', 'lag_1', 'value'\n",
        "```\n",
        "\n",
        "最後一個欄位 <font color='brown'>‘value’</font> 是本案例需要預測的對象，是指未來一定期間內客戶購買的金額，也就是所謂的「貢獻值」。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2VP7Rva98Jr"
      },
      "source": [
        "### 資料命名規則"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD2VZX_c9D25"
      },
      "source": [
        "以下使用兩個dataset示範與練習\n",
        "- CLV Case\n",
        "    - train: 訓練集載入 DataFrame\n",
        "    - test: 測試集載入 DataFrame\n",
        "    - X_train, y_train (clv case 訓練集)\n",
        "    - X_test, y_test (clv case 測試集)\n",
        "- load_boston()\n",
        "    - X_bos, y_bos (load_boston() 資料)\n",
        "    - bos (資料打包為 Pandas DataFrame)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSDJEjVqI2q9"
      },
      "source": [
        "#存一份資料副本，以便後續使用\n",
        "train_csv = os.path.join(data_dir,\"ec_201012_train_4.csv\")\n",
        "test_csv = os.path.join(data_dir,\"ec_201012_test_4.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iOBNLbkbnUN"
      },
      "source": [
        "train = pd.read_csv(train_csv)\n",
        "test = pd.read_csv(test_csv)\n",
        "\n",
        "#製作train test data\n",
        "x_train = train.drop(['CustomerID','value'],axis = 1)\n",
        "y_train = train.value\n",
        "\n",
        "x_test = test.drop(['CustomerID','value'],axis = 1)\n",
        "y_test = test.value\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_ZXszemb4Lo"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "\n",
        "data = load_boston() #載入boston房價資料\n",
        "bos = pd.DataFrame(data = data['data'],\n",
        "                   columns = data['feature_names'])\n",
        "\n",
        "bos['y']= data['target']\n",
        "x_bos = bos.drop(['y'],axis =1)\n",
        "y_bos = bos.y\n",
        "\n",
        "# from sklearn.datasets import load_boston\n",
        "\n",
        "# data = load_boston()#載入boston房價資料\n",
        "# bos = pd.DataFrame(data = data['data'], \n",
        "#     columns = data['feature_names'])\n",
        "# bos['y'] = data['target']\n",
        "# X_bos = bos.drop(['y'], axis = 1)\n",
        "# y_bos = bos.y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1eTh-RTR2Vq"
      },
      "source": [
        "#Boston dataset欄位定義\n",
        "\n",
        "- CRIM per capita crime rate by town\n",
        "\n",
        "- ZN proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "\n",
        "- INDUS proportion of non-retail business acres per town\n",
        "\n",
        "- CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "\n",
        "- NOX nitric oxides concentration (parts per 10 million)\n",
        "\n",
        "- RM average number of rooms per dwelling\n",
        "\n",
        "- AGE proportion of owner-occupied units built prior to 1940\n",
        "\n",
        "- DIS weighted distances to five Boston employment centres\n",
        "\n",
        "- RAD index of accessibility to radial highways\n",
        "\n",
        "- TAX full-value property-tax rate per $10,000\n",
        "\n",
        "- PTRATIO pupil-teacher ratio by town\n",
        "\n",
        "- B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
        "\n",
        "- LSTAT % lower status of the population\n",
        "\n",
        "- MEDV Median value of owner-occupied homes in $1000’s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZvl066tVOmG"
      },
      "source": [
        "## 常常需要載入的 Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdzGtTXyhJSZ"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "#是先定義好所需的函式，利後續重複使用\n",
        "def simple_ols(xvec, yvec):\n",
        "    Xadd = sm.add_constant(xvec)\n",
        "    model = sm.OLS(yvec, Xadd).fit()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SndRnJxYldI8"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg9dO7Keu3Jh"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA6irommVnGz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDzxtLRZVjhK"
      },
      "source": [
        "#檢視欄位(features)是否有缺失值\n",
        "train.isnull().sum()\n",
        "test.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLZG2R6NV3KH"
      },
      "source": [
        "#檢視欄位的關聯性，用以確認選取哪些欄位\n",
        "cor = train.corr()\n",
        "print(cor.value.sort_values(ascending=True))#直接他出value值的關聯性\n",
        "\n",
        "#欄位太多造成熱圖失效\n",
        "fig,ax = plt.subplots(figsize = (20,10))\n",
        "\n",
        "sns.heatmap(cor,xticklabels = cor.columns,\n",
        "            yticklabels = cor.columns, annot=True,\n",
        "            cmap = 'RdBu', ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_rALEp7c4tu"
      },
      "source": [
        "## Customter Lifetime Value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuJ0oEHvYBvm"
      },
      "source": [
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "\n",
        "the_model = make_pipeline(\n",
        "    #先將資料standardization\n",
        "    #然後將standardiztion的資料丟入linearRegression\n",
        "    StandardScaler(),\n",
        "    LinearRegression()\n",
        ")\n",
        "\n",
        "#將global_var assign給 local_var\n",
        "train_x = x_train\n",
        "train_y = y_train\n",
        "\n",
        "test_x = x_test\n",
        "test_y = y_test\n",
        "\n",
        "the_model.fit(train_x,train_y)\n",
        "y_pred = the_model.predict(test_x)\n",
        "\n",
        "#檢驗模型預的結果，使用指標：rmse(root-mean squred error)\n",
        "rmse = np.sqrt(mean_squared_error(test_y,y_pred))\n",
        "r2 = r2_score(test_y,y_pred)\n",
        "r2,rmse#得出r^2:0.6182389801675541 rmse:206.16901350645512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx8NTkUVgCkY"
      },
      "source": [
        "#To show what we got\n",
        "#會員資料主要包四個面向：date(時間),basket_value(購物車金額),basket_size(購物車商品數),lag(過往的clv)\n",
        "print(train_x.columns)#columns contain in x\n",
        "print(train_x.iloc[:,:4])\n",
        "\n",
        "date_data_set =train_x.filter(regex=\"date\")\n",
        "value_data_set =train_x.filter(regex=\"baseket_value\")\n",
        "size_data_set =train_x.filter(regex=\"basket_size\")\n",
        "lag_data_set =train_x.filter(regex=\"lag\")\n",
        "\n",
        "cor_date = date_data_set.corr()\n",
        "cor_value = value_data_set.corr()\n",
        "cor_size = size_data_set.corr()\n",
        "cor_lag = lag_data_set.corr()\n",
        "\n",
        "\n",
        "fig,ax = plt.subplots(2,2,figsize=(50,50))\n",
        "sns.set(font_scale=2.5) \n",
        "sns.heatmap(cor_date,xticklabels = cor_date,\n",
        "            yticklabels = cor_date, annot=True,\n",
        "            cmap = 'copper_r', ax=ax[0][0])\n",
        "\n",
        "sns.heatmap(cor_value,xticklabels = cor_value,\n",
        "            yticklabels = cor_value, annot=True,\n",
        "            cmap = 'copper_r', ax=ax[0][1])\n",
        "\n",
        "sns.heatmap(cor_size,xticklabels = cor_size,\n",
        "            yticklabels = cor_size, annot=True,\n",
        "            cmap = 'copper_r', ax=ax[1][0])\n",
        "\n",
        "sns.heatmap(cor_lag,xticklabels = cor_lag,\n",
        "            yticklabels = cor_lag, annot=True,\n",
        "            cmap = 'copper_r', ax=ax[1][1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(train_y.sort_values(ascending=False)[:10])#y is the clv of each customer\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJXoRAT8eu1l"
      },
      "source": [
        "#use scatter plot to visualize how data(y_pred,test_y) distribute\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.regplot(y_pred,test_y)\n",
        "plt.title(\"test_y and test_y regplot\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB_2LTHwnWej"
      },
      "source": [
        "## load_boston"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcpMR7EsaPeQ"
      },
      "source": [
        "#boston data develope\n",
        "cor_bos = bos.corr()\n",
        "cor_bos.y.sort_values(ascending=True)\n",
        "\n",
        "fig,ax = plt.subplots(figsize = (20,10))\n",
        "sns.heatmap(cor_bos,xticklabels=bos.columns,yticklabels=bos.columns,\n",
        "            cmap=\"summer\",annot= True,ax=ax)\n",
        "#較不相關的變數屬於接近0的值"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRL9rYctaDNg"
      },
      "source": [
        "train_x,test_x,train_y,test_y = \\\n",
        "      train_test_split(x_bos,y_bos,test_size = 0.2)\n",
        "\n",
        "\n",
        "model = make_pipeline(\n",
        "    MinMaxScaler(),\n",
        "    LinearRegression()\n",
        ")\n",
        "\n",
        "model.fit(train_x, train_y)\n",
        "y_pred = model.predict(test_x)\n",
        "rmse = np.sqrt(mean_squared_error(test_y,y_pred))\n",
        "r_2 = r2_score(test_y,y_pred)\n",
        "print(\"way1\",r_2,rmse)\n",
        "\n",
        "#使用stats package 能較快數計算出統計指標\n",
        "from statsmodels.tools.eval_measures import rmse\n",
        "model = simple_ols(x_bos,y_bos)\n",
        "rmse_score = rmse(y_bos, model.fittedvalues)\n",
        "\n",
        "print(\"way2:\",r_2, rmse_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZxOBClj-aTB"
      },
      "source": [
        "# Charting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43HmKubnB7CS"
      },
      "source": [
        "def start_plot(figsize=(10, 8), style = 'whitegrid'):\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    gs = fig.add_gridspec(1,1)\n",
        "    plt.tight_layout()\n",
        "    with sns.axes_style(style):\n",
        "        ax = fig.add_subplot(gs[0,0])\n",
        "    return ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNUxiURdustX"
      },
      "source": [
        "## 誤差值的統計分佈模型 Distribution of Residuals\n",
        "\n",
        "- 功用：\n",
        "- 分析面向："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW1tQqGlOw_w"
      },
      "source": [
        "- Points are **independent** of each other (residuals are uncorrelated)\n",
        "- <font color='brown'>**residual ε are normally distributed with  μ = 0**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keLosDetvvVM"
      },
      "source": [
        "#了解residual的分佈，判斷預測值與真\n",
        "from sklearn.linear_model import LinearRegression\n",
        "#先訓練所需模型，在找出預測值\n",
        "model = LinearRegression()\n",
        "model.fit(train_x,train_y)\n",
        "pred_y = model.predict(test_x)\n",
        "\n",
        "#計算出預測值與實際值的差距就可得殘差項\n",
        "ax=start_plot(figsize=(10,10))\n",
        "sns.distplot(test_y-pred_y, bins = 20,color='orange',ax=ax)\n",
        "ax.axvline(x= 0,ls = \"--\",c= 'brown')#殘差偏離x=0表示欄位間可能不互為獨立"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny3IXbvMxjcB"
      },
      "source": [
        "#製作殘差排序曲線圖（sorted-residual plot）\n",
        "residual = (test_y - pred_y).sort_values(ascending=True)\n",
        "ax = start_plot(figsize=(10,10))\n",
        "ax.plot(range(len(residual)),residual)\n",
        "ax.axhline(y=0,ls='--',c='red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWxchr24ngMa"
      },
      "source": [
        "## 預測值的範圍與分佈"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XpUYI9W2Tp3"
      },
      "source": [
        "print(pred_y)\n",
        "print(test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmz0qK1Okhzs"
      },
      "source": [
        "#初步檢視預測值與實際值的分佈\n",
        "model = LinearRegression()\n",
        "model.fit(train_x,train_y)\n",
        "pred_y = model.predict(test_x)\n",
        "true_y = test_y\n",
        "ax = start_plot(figsize=(10,10),style = 'darkgrid')\n",
        "ax.plot(pred_y,label=\"Obsevation\")\n",
        "ax.plot(true_y,label=\"Prediction\")\n",
        "ax.legend(frameon=True,fontsize = 14,shadow=True)\n",
        "#此圖並無法顯示分佈情形"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFiNPxOW3NTb"
      },
      "source": [
        "#方法一 ：seaborn繪製whiskerboxplot:運用dataframe方式同時繪製兩個箱形圖並進行比較分布差異\n",
        "ndf = pd.DataFrame()\n",
        "ndf['Obsevation'] = pred_y\n",
        "ndf['Prediction'] = test_y\n",
        "ax = start_plot(figsize=(10,10))\n",
        "sns.boxplot(data=ndf,orient=\"h\",ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YFAaYuy4BdC"
      },
      "source": [
        "#方法二 ：使用分佈圖比較兩者的分佈差異\n",
        "ax=start_plot(figsize=(8,6))\n",
        "sns.distplot(ndf.Obsevation)\n",
        "sns.distplot(ndf.Prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYb4ZR_KJlDi"
      },
      "source": [
        "## $ y  - \\hat y$ (預測效果) 的分析 （Regression Prediction Error）\n",
        "\n",
        "- 功用：顯示預測值與實際值的散佈圖\n",
        "\n",
        "- 分析面向：實際的模型預測與百分之百正確間的差異，可以顯示目前資料所製作的預測值是偏高或偏低，再進行調整\n",
        "\n",
        "- [YellowBrick PredictionErrorPlot](https://www.scikit-yb.org/en/latest/api/regressor/peplot.html)\n",
        "\n",
        "![texto alternativo](https://www.scikit-yb.org/en/latest/api/regressor/peplot-1.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSqRzk4HuhHL"
      },
      "source": [
        "!pip3 install --upgrade yellowbrick"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOJ6poGrqFLt"
      },
      "source": [
        "\n",
        "model= LinearRegression()\n",
        "model.fit(train_x,train_y)\n",
        "ax=start_plot(figsize=(20,10),style=\"darkgrid\")\n",
        "pred_y = model.predict(test_x)\n",
        "sns.regplot(test_y,pred_y)\n",
        "# 調整座標軸，讓圖型成正方形且兩軸刻度一致\n",
        "#作法：將兩軸的最大/最小值調整成一致(為避免資料點貼著軸線，故加減做微調)\n",
        "lim1 = min(min(test_y),min(pred_y))-50\n",
        "lim2 = max(max(test_y),max(pred_y))+50\n",
        "lim = [lim1,lim2]\n",
        "ax.set_xlim(lim)\n",
        "ax.set_ylim(lim)\n",
        "#強化資料點的輪廓\n",
        "plt.scatter(test_y,pred_y,c='green',edgecolors=\"navy\",label=r\"$'r^2=%.4f$\"%r2_score(test_y,pred_y))\n",
        "#加上對角線虛線並顯示其意義\n",
        "ax.plot(lim,lim,ls=\"--\",linewidth=5,c=\"orange\",label = r'$y = \\haty, identity$')\n",
        "#加上圖例\n",
        "ax.legend(loc='best',frameon=True,shadow=True,fontsize=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFG9S7ZlUyPX"
      },
      "source": [
        "## Residuals Analysis 錯誤值的分析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr4U-cDjk8lv"
      },
      "source": [
        "- [Residuals Plot](https://www.scikit-yb.org/en/latest/api/regressor/residuals.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfF85aWukynl"
      },
      "source": [
        "### My Residuals Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIOha5GOzZIS"
      },
      "source": [
        "plt.style.available"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z1JC2Jkxoa7"
      },
      "source": [
        "model = LinearRegression()\n",
        "model.fit(train_x,train_y)\n",
        "pred_train_y = model.predict(train_x)\n",
        "pred_test_y = model.predict(test_x)\n",
        "\n",
        "train_residual = train_y - pred_train_y\n",
        "test_residual = test_y - pred_test_y\n",
        "\n",
        "train_r2 = r2_score(train_y,pred_train_y)\n",
        "test_r2 = r2_score(test_y,pred_test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7u1OYFzybOs"
      },
      "source": [
        "plt.style.use(\"seaborn-notebook\")\n",
        "ax= start_plot(figsize=(20,10),style=\"darkgrid\")\n",
        "\n",
        "sns.regplot(train_y,train_residual)\n",
        "sns.regplot(test_y,test_residual)\n",
        "plt.scatter(train_y,train_residual,edgecolors=\"navy\",label=\"Train Residual \" + \\\n",
        "            r'$r^2:%.4f$' % train_r2)\n",
        "plt.scatter(test_y,test_residual,edgecolors='black',label=\"Test Residual \" + \\\n",
        "            r'$r^2:%.4f$' % test_r2)\n",
        "\n",
        "\n",
        "ax.axhline(y= 0,ls=\"--\",c=\"darkslategray\")\n",
        "ax.legend(loc = \"best\",frameon=True,fancybox=True,shadow=True,fontsize=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqJZREijuyCk"
      },
      "source": [
        "## 學習曲線"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIbs4EpTlmZ_"
      },
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "#\n",
        "# Simplified version of plot_learning_curves presented in \n",
        "#   https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
        "#\n",
        "\n",
        "def my_plot_leaning_curves(model, X, y, cv=None, scoring = None,\n",
        "        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10)):\n",
        "    \n",
        "       \n",
        "    fig,ax = plt.subplots(figsize = (10, 8))\n",
        "    ax.set_xlabel(\"Training examples\")\n",
        "    ax.set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
        "        learning_curve(model, X, y, cv=cv, n_jobs=n_jobs, \n",
        "            scoring = scoring,\n",
        "            train_sizes=train_sizes,\n",
        "            return_times=True)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    fit_times_mean = np.mean(fit_times, axis=1)\n",
        "    fit_times_std = np.std(fit_times, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "\n",
        "\n",
        "    ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    ax.grid(b = 'on', ls = '--', alpha = 0.8)\n",
        "    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "\n",
        "    ax.legend(loc=\"best\", fontsize = 14, shadow =True, frameon = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV4yxqZAlt_y"
      },
      "source": [
        "my_plot_leaning_curves(LinearRegression(), X_bos, y_bos,\n",
        "    train_sizes=np.linspace(.1, 1.0, 5),\n",
        "    scoring='neg_root_mean_squared_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u93ZiaE-g2R"
      },
      "source": [
        "# Metrics & Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oAAhSNYbqUM"
      },
      "source": [
        "## Cook's Distance\n",
        "1.用以評估每個資料點對於mse的影響度\n",
        "\n",
        "2.cook's distance大，表示資料點可能為離群值，故今其排除可提升模型的解釋能力\n",
        "\n",
        "3.一般以整體cook's distance的三倍為界線，大於三倍則是為cook's distance大"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbU3h--uksU8"
      },
      "source": [
        "- [Wikipedia](https://en.wikipedia.org/wiki/Cook%27s_distance)\n",
        "- [Mathworks](https://www.mathworks.com/help/stats/cooks-distance.html#:~:text=Cook's%20distance%20is%20the%20scaled,on%20the%20fitted%20response%20values.)\n",
        "- [Stackoverflow QA](https://stackoverflow.com/questions/51390196/how-to-calculate-cooks-distance-dffits-using-python-statsmodel)\n",
        "\n",
        "- [MAPE formula](https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn/294069#294069)\n",
        "\n",
        "- [Issue 15007](https://github.com/scikit-learn/scikit-learn/pull/15007)\n",
        "\n",
        ">Simply said, Cook’s D is calculated by removing the ith data point from the model and recalculating the regression. All the values in the regression model are then observed whether changes have been detected after the removal of the point. This is an iterative way of examining the influence of that observation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQz9eK2lnsqg"
      },
      "source": [
        "以 loas_boston 為例：\n",
        "\n",
        "```\n",
        "model = simple_ols(X_bos, y_bos)\n",
        "infl = model.get_influence()\n",
        "rdf = infl.summary_frame()\n",
        "cooks_d = rdf['cooks_d']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AropacQmhMtd"
      },
      "source": [
        "#視覺化Cook's distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sriLrdNfZv-e"
      },
      "source": [
        "\n",
        "model = simple_ols(x_bos,y_bos)\n",
        "inf1 = model.get_influence()#statmodle package現成的function\n",
        "rdf = inf1.summary_frame()\n",
        "cooks_d = rdf['cooks_d']\n",
        "ax = start_plot(figsize=(20,10))\n",
        "ax.vlines(range(len(rdf)),cooks_d,0,color='navy',label=\"Cook's distance\")\n",
        "ax.legend(frameon = True, fontsize = 14, shadow = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDTtARYsiev6"
      },
      "source": [
        "cooks_d = rdf['cooks_d'].sort_values(ascending = True)\n",
        "# type(cooks_d), cooks_d\n",
        "\n",
        "#抓出top8 cook's distance value\n",
        "top8 = list(cooks_d.index[:int(len(cooks_d)*0.08)])\n",
        "\n",
        "#抓出大於cook's distance 平均值三倍的value\n",
        "mu3 = 3*cooks_d.mean()\n",
        "above3 = list(cooks_d.loc[cooks_d > mu3].index)\n",
        "\n",
        "#製作兩個資料集：drop掉mu3與drop掉top8\n",
        "bos2 = bos.drop(above3,axis = 0)\n",
        "bos3 = bos.drop(top8,axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCy7NFAOkV4O"
      },
      "source": [
        "#比較排除前與排除後cross_val_score差異\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(LinearRegression(), x_bos,y_bos)\n",
        "scores2 = cross_val_score(LinearRegression(),bos2.drop(['y'],axis =1),bos2.y)\n",
        "scores3 = cross_val_score(LinearRegression(),bos3.drop(['y'],axis =1),bos3.y)\n",
        "\n",
        "\n",
        "print(\"score of bos: \",scores,\";\",scores.mean(),\";\",scores.std())\n",
        "print(\"score of bos2: \",scores2,\";\",scores2.mean(),\";\",scores2.std())\n",
        "print(\"score of bos3: \",scores3,\";\",scores3.mean(),\";\",scores3.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqkrcuPGBLBz"
      },
      "source": [
        "- [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
        "- [What Is R Squared And Negative R Squared](http://www.fairlynerdy.com/what-is-r-squared/)\n",
        "- [Can the multiple linear correlation coefficient be negative?](https://stats.stackexchange.com/questions/6181/can-the-multiple-linear-correlation-coefficient-be-negative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nl90yxICT1c"
      },
      "source": [
        "### yellowbrick 版本"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk6Z3-f2BALK"
      },
      "source": [
        "```\n",
        "from yellowbrick.regressor import CooksDistance\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "data = load_boston()\n",
        "X, y = data['data'], data['target']\n",
        "\n",
        "# Instantiate and fit the visualizer\n",
        "visualizer = CooksDistance()\n",
        "visualizer.fit(X, y)\n",
        "visualizer.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gx3H-m0Hv1x"
      },
      "source": [
        "## VIF(變異數膨脹因子)\n",
        "\n",
        ">Variance Inflation Factor (VIF) is used to detect the presence of multicollinearity. Variance inflation factors (VIF) measure how much the variance of the estimated regression coefficients are inflated as compared to when the predictor variables are not linearly related.\n",
        "\n",
        ">- 功用：計算兩兩欄位間的關聯性 --> 防止共線性發生"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCIkkgduuLiI"
      },
      "source": [
        "- [select_dtypes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html)\n",
        "- [endog, exog, what’s that?](https://www.statsmodels.org/stable/endog_exog.html)\n",
        "- [variance_inflation_factor](https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02NtmhsBwXdW"
      },
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "df=bos.select_dtypes(include=[np.number])\n",
        "bos.select_dtypes(include=[np.number]).shape\n",
        "range(df.shape[1])\n",
        "\n",
        "[variance_inflation_factor(df.values,1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Iv0UEL-H3eD"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from patsy import dmatrices\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "def vif_scores(df):\n",
        "    df.dropna()\n",
        "    # df = df._get_numeric_data()\n",
        "    df = df.select_dtypes(include=[np.number])#篩選出數執行資料\n",
        "    vif = pd.DataFrame()#另外創一個dataframe\n",
        "\n",
        "    vif[\"VIF Factor\"] = \\\n",
        "        [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
        "\n",
        "    vif[\"features\"] = df.columns\n",
        "    return vif\n",
        "\n",
        "def my_mape(estimator, X, y): \n",
        "    estimator.fit(X, y)\n",
        "    y_pred = estimator.predict(X)\n",
        "    y_true = y\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI7qKaAW2an0"
      },
      "source": [
        "bos2 = bos.copy()\n",
        "vif = vif_scores(bos2.drop(['y'],axis=1))\n",
        "vif_sort_idx = list(vif['VIF Factor'].sort_values(ascending=False).index)\n",
        "\n",
        "#拿掉前三大VIF的變數\n",
        "bos2 = bos2.drop(vif.iloc[vif_sort_idx[:3]]['features'],axis=1)\n",
        "# bos2.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iwivbTM33fu"
      },
      "source": [
        "#比較排除前與排除後cross_val_score差異\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(LinearRegression(), x_bos,y_bos)\n",
        "scores2 = cross_val_score(LinearRegression(),bos2.drop(['y'],axis =1),bos2.y)\n",
        "\n",
        "\n",
        "\n",
        "print(\"score of bos: \",scores,\";\",scores.mean(),\";\",scores.std())\n",
        "print(\"score of bos2: \",scores2,\";\",scores2.mean(),\";\",scores2.std())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHjC6rAfDQgW"
      },
      "source": [
        " # Finalize and render the figure\n",
        "from yellowbrick.regressor import ResidualsPlot\n",
        "\n",
        "model = LinearRegression()\n",
        "visualizer = ResidualsPlot(model)\n",
        "\n",
        "visualizer.fit(train_x, train_y)  # Fit the training data to the visualizer\n",
        "visualizer.score(test_x, test_y)  # Evaluate the model on the test data\n",
        "visualizer             "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed8tRjZ2-kub"
      },
      "source": [
        "# 降維"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69J1YM2la32t"
      },
      "source": [
        "## Principal Component Analysis\n",
        "\n",
        "- 共用：用數學工具(線性代數)在資料統計特性不影響下，減少資料維度(欄位)\n",
        "\n",
        "\n",
        "- [線代啟示錄：主成分分析](https://ccjou.wordpress.com/2013/04/15/主成分分析/)\n",
        "- [機器/統計學習:主成分分析(Principal Component Analysis, PCA)](https://medium.com/@chih.sheng.huang821/機器-統計學習-主成分分析-principle-component-analysis-pca-58229cd26e71)\n",
        "- [主成分分析的原理](http://web.ntpu.edu.tw/~ccw/statmath/M_pca.pdf)\n",
        "- [如何通俗易懂地讲解什么是 PCA 主成分分析？](https://www.zhihu.com/question/41120789)\n",
        "- [scikit-learn PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
        "- [scikit-learn User Guide: 2.5. Decomposing signals in components (matrix factorization problems)](https://scikit-learn.org/stable/modules/decomposition.html#pca)\n",
        "\n",
        "- [Principal Component Analysis (PCA) in Python](https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python)\n",
        "- [Feature Selection Techniques in Machine Learning with Python](https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVwPfNEOC_KL"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIrbrBv5TyVZ"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# model = LinearRegression()\n",
        "#首先執行上述流程：建模、訓練、測試\n",
        "\n",
        "model = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    LinearRegression()\n",
        ")\n",
        "\n",
        "model.fit(train_x,train_y)\n",
        "\n",
        "# #接著計算模型預測的評估分數：RMSE、R2\n",
        "pred_train_y = model.predict(train_x)\n",
        "pred_test_y = model.predict(test_x)\n",
        "\n",
        "train_rmse =  -np.sqrt(mean_squared_error(train_y,pred_train_y))\n",
        "test_rmse =  -np.sqrt(mean_squared_error(test_y,pred_train_y))\n",
        "\n",
        "train_r2 = model.score(train_x,train_y)\n",
        "test_r2 = model.score(test_x,test_y)\n",
        "\n",
        "print(\"training: R2 = %.4f; rmse = %.4f\" % (train_r2,train_rmse)\n",
        ")\n",
        "print(\"testing: R2 = %.4f; rmse = %.4f\" % (test_r2,test_rmse))\n",
        "\n",
        "# model = make_pipeline(\n",
        "#     StandardScaler(),\n",
        "#     LinearRegression()\n",
        "# )\n",
        "\n",
        "# model.fit(X_train, y_train)\n",
        "\n",
        "# yhat_train = model.predict(X_train)\n",
        "# yhat_test = model.predict(X_test)\n",
        "\n",
        "\n",
        "# TRAINING_RMSE = -np.sqrt(mean_squared_error(y_train, yhat_train))\n",
        "# TEST_RMSE = -np.sqrt(mean_squared_error(y_test, yhat_test))\n",
        "\n",
        "# TRAINING_SCORE = model.score(X_train, y_train)\n",
        "# TEST_SCORE = model.score(X_test, y_test)\n",
        "\n",
        "# print('training: score = %.4f, neg rmse = %.4f' % \n",
        "#       (TRAINING_SCORE, TRAINING_RMSE))\n",
        "# # yhat = lin.predict(X_test)\n",
        "# print('test: score = %.4f, neg rmse = %.4f' % \n",
        "#       (TEST_SCORE, TEST_RMSE))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idn8C4PHpiAb"
      },
      "source": [
        "import logging\n",
        "logging.getLogger(\"imported_module\").setLevel(logging.INFO)\n",
        "#預設資料欄位數\n",
        "N = len(train_x.columns)\n",
        "\n",
        "#create 一個等長、0組成的np.array\n",
        "train_score = np.zeros(N)\n",
        "test_score = np.zeros(N)\n",
        "\n",
        "train_rmse = np.zeros(N)\n",
        "test_rmse = np.zeros(N)\n",
        "\n",
        "#make_pipepline串接資料格式：依照n(由多至少)的對資料進行降維，隨後丟入model進行訓練與測試\n",
        "for i in np.arange(N,2,-1):\n",
        "  model = make_pipeline(\n",
        "      StandardScaler(),\n",
        "      #針對資料做降維處理(降成n維)\n",
        "      PCA(n_components=i),\n",
        "      LinearRegression()\n",
        "  )\n",
        "\n",
        "  model.fit(train_x,train_y)\n",
        "  #在降維後訓練並得到score，以做比較各維度的模型分數：R2、RMSE\n",
        "  train_score[i-1] = model.score(train_x,train_y)\n",
        "  test_score[i-1] = model.score(test_x,test_y)\n",
        "\n",
        "  pred_train_y = model.predict(train_x)\n",
        "  pred_test_y = model.predict(test_x)\n",
        "\n",
        "  train_rmse[i-1] = -np.sqrt(mean_squared_error(train_y,pred_train_y))\n",
        "  test_rmse[i-1] = -np.sqrt(mean_squared_error(test_y,pred_test_y))\n",
        "\n",
        "\n",
        "#根據圖形結果分析：在test_set R2趨勢中，n=25,26達到相對高點;RMSE同時達相對低點\n",
        "#以n=25,26的維度進行standardization後的linearregression model可以達到較優的表現\n",
        "x_domain = np.arange(2,N,1)\n",
        "\n",
        "fig,ax=plt.subplots(2,1,figsize=(20,10))\n",
        "\n",
        "train_max=sum(train_score[2:]!=max(train_score[2:])) \n",
        "test_max=sum(test_score[2:]!=max(train_score[2:])) \n",
        "ax[0].plot(x_domain,train_score[2:],color=\"darkorange\",label=\"Training Score\")\n",
        "ax[0].plot(x_domain,test_score[2:],color=\"darkred\",label=\"Testing Score\")\n",
        "\n",
        "ax[0].vlines(test_max,ymin=0,ymax=max(test_score[2:]),linestyles='--',color='grey')\n",
        "ax[0].vlines(train_max,ymin=0,ymax=max(train_score[2:]),linestyles='--',color='grey')\n",
        "\n",
        "ax[0].legend(loc=\"best\",frameon = True,fontsize=14,shadow=True)\n",
        "ax[0].set_title(\"Score Trending\",size=20)\n",
        "ax[0].set_xticks(x_domain);#tips:\n",
        "              #use a semi-colon after the plot command\n",
        "              #will display the graph and stop the verbose\n",
        "              #also,plt.show() supress unnecessary variable\n",
        "              #https://stackoverflow.com/questions/12056115/disable-the-output-of-matplotlib-pyplot\n",
        "\n",
        "\n",
        "# train_max=sum(train_rmse[2:]!=max(train_rmse[2:])) \n",
        "# test_max=sum(test_rmse[2:]!=max(train_rmse[2:])) \n",
        "\n",
        "ax[1].plot(x_domain,train_rmse[2:],color=\"darkgreen\",label=\"Training RMSE\")\n",
        "ax[1].plot(x_domain,test_rmse[2:],color=\"darkblue\",label=\"Testing RMSE\")\n",
        "\n",
        "# ax[1].vlines(train_max,ymin=max(train_rmse[2:]),ymax=0,linestyles='--',color='grey')\n",
        "# ax[1].vlines(test_max,ymin=max(test_rmse[2:]),ymax=0,linestyles='--',color='grey')\n",
        "\n",
        "ax[1].legend(loc=\"best\",frameon = True,fontsize=14,shadow=True)\n",
        "ax[1].set_title(\"RMSE Trending\",size=20)\n",
        "ax[1].set_xticks(x_domain);\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkpuI2AGLTrC"
      },
      "source": [
        "### Give it a try"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGe8gDUFLWp1"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def PCA_analysis(estimater,preprocessing, n_features, X_train, y_train, \n",
        "    X_test = None, y_test = None):\n",
        "\n",
        "    N = n_features\n",
        "\n",
        "    scores_train = np.zeros(N)\n",
        "    scores_test = np.zeros(N)\n",
        "\n",
        "    for i in np.arange(N, 2, -1):\n",
        "        model = make_pipeline(\n",
        "            preprocessing,\n",
        "            PCA(n_components = i),\n",
        "            estimater\n",
        "        )\n",
        "        model.fit(X_train, y_train)\n",
        "        scores_train[i-1] = model.score(X_train, y_train)\n",
        "        if X_test is not None:\n",
        "            scores_test[i-1] = model.score(X_test, y_test)\n",
        "\n",
        "    xdomain = np.arange(2, N, 1)\n",
        "    ax = start_plot(figsize=(10,7))\n",
        "    ax.plot(xdomain, scores_train[2:], label = 'Training Scores')\n",
        "    if X_test is not None:\n",
        "        ax.plot(xdomain, scores_test[2:], color = 'brown',\n",
        "            label = 'Test Scores')\n",
        "    ax.legend(loc='lower center', frameon=True,shadow=True,fancybox=True,fontsize=14)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBUv6Gaz0qTs"
      },
      "source": [
        "train_x.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhY9GltMe1cF"
      },
      "source": [
        "PCA_analysis(LinearRegression(),MinMaxScaler(),train_x.shape[1],train_x,train_y,test_x,test_y)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}