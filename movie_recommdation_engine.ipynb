{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "movie recommdation engine",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNV11JgqFk+Jl/5Iz30cg2F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jerremiah/project-set/blob/main/movie_recommdation_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ApdecUFqbla"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipINSj5Uq5e5"
      },
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kftRLOhpqMHG"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "#___________________________\n",
        "def load_tmdb_movies(path):\n",
        "    df = pd.read_csv(path)\n",
        "    df['release_date'] = pd.to_datetime(df['release_date']).apply(lambda x: x.date())\n",
        "    json_columns = ['genres', 'keywords', 'production_countries',\n",
        "                    'production_companies', 'spoken_languages']\n",
        "    for column in json_columns:\n",
        "        df[column] = df[column].apply(json.loads)\n",
        "    return df\n",
        "#___________________________\n",
        "def load_tmdb_credits(path):\n",
        "    df = pd.read_csv(path)\n",
        "    json_columns = ['cast', 'crew']\n",
        "    for column in json_columns:\n",
        "        df[column] = df[column].apply(json.loads)\n",
        "    return df\n",
        "#___________________\n",
        "LOST_COLUMNS = [\n",
        "    'actor_1_facebook_likes',\n",
        "    'actor_2_facebook_likes',\n",
        "    'actor_3_facebook_likes',\n",
        "    'aspect_ratio',\n",
        "    'cast_total_facebook_likes',\n",
        "    'color',\n",
        "    'content_rating',\n",
        "    'director_facebook_likes',\n",
        "    'facenumber_in_poster',\n",
        "    'movie_facebook_likes',\n",
        "    'movie_imdb_link',\n",
        "    'num_critic_for_reviews',\n",
        "    'num_user_for_reviews']\n",
        "#____________________________________\n",
        "TMDB_TO_IMDB_SIMPLE_EQUIVALENCIES = {\n",
        "    'budget': 'budget',\n",
        "    'genres': 'genres',\n",
        "    'revenue': 'gross',\n",
        "    'title': 'movie_title',\n",
        "    'runtime': 'duration',\n",
        "    'original_language': 'language',\n",
        "    'keywords': 'plot_keywords',\n",
        "    'vote_count': 'num_voted_users'}\n",
        "#_____________________________________________________\n",
        "IMDB_COLUMNS_TO_REMAP = {'imdb_score': 'vote_average'}\n",
        "#_____________________________________________________\n",
        "def safe_access(container, index_values):\n",
        "    # return missing value rather than an error upon indexing/key failure\n",
        "    result = container\n",
        "    try:\n",
        "        for idx in index_values:\n",
        "            result = result[idx]\n",
        "        return result\n",
        "    except IndexError or KeyError:\n",
        "        return pd.np.nan\n",
        "#_____________________________________________________\n",
        "def get_director(crew_data):\n",
        "    directors = [x['name'] for x in crew_data if x['job'] == 'Director']\n",
        "    return safe_access(directors, [0])\n",
        "#_____________________________________________________\n",
        "def pipe_flatten_names(keywords):\n",
        "    return '|'.join([x['name'] for x in keywords])\n",
        "#_____________________________________________________\n",
        "def convert_to_original_format(movies, credits):\n",
        "    tmdb_movies = movies.copy()\n",
        "    tmdb_movies.rename(columns=TMDB_TO_IMDB_SIMPLE_EQUIVALENCIES, inplace=True)\n",
        "    tmdb_movies['title_year'] = pd.to_datetime(tmdb_movies['release_date']).apply(lambda x: x.year)\n",
        "    # I'm assuming that the first production country is equivalent, but have not been able to validate this\n",
        "    tmdb_movies['country'] = tmdb_movies['production_countries'].apply(lambda x: safe_access(x, [0, 'name']))\n",
        "    tmdb_movies['language'] = tmdb_movies['spoken_languages'].apply(lambda x: safe_access(x, [0, 'name']))\n",
        "    tmdb_movies['director_name'] = credits['crew'].apply(get_director)\n",
        "    tmdb_movies['actor_1_name'] = credits['cast'].apply(lambda x: safe_access(x, [1, 'name']))\n",
        "    tmdb_movies['actor_2_name'] = credits['cast'].apply(lambda x: safe_access(x, [2, 'name']))\n",
        "    tmdb_movies['actor_3_name'] = credits['cast'].apply(lambda x: safe_access(x, [3, 'name']))\n",
        "    tmdb_movies['genres'] = tmdb_movies['genres'].apply(pipe_flatten_names)\n",
        "    tmdb_movies['plot_keywords'] = tmdb_movies['plot_keywords'].apply(pipe_flatten_names)\n",
        "    return tmdb_movies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdI7IPYeOMwp"
      },
      "source": [
        "1. Exploration\n",
        "\n",
        "1.1 Keywords\n",
        "1.2 Filling factor: missing values\n",
        "1.3 Number of films per year\n",
        "1.4 Genres\n",
        "2. Cleaning\n",
        "\n",
        "2.1 Cleaning of the keywords\n",
        "2.1.1 Grouping by roots\n",
        "2.1.2 Groups of synonyms\n",
        "2.2 Correlations\n",
        "2.3 Missing values\n",
        "2.3.1 Setting missing title years\n",
        "2.3.2 Extracting keywords from the title\n",
        "2.3.3 Imputing from regressions\n",
        "3. Recommendation Engine\n",
        "\n",
        "3.1 Basic functioning of the engine\n",
        "3.1.1 Similarity\n",
        "3.1.2 Popularity\n",
        "3.2 Definition of the recommendation engine functions\n",
        "3.3 Making meaningfull recommendations\n",
        "3.4 Exemple of recommendation: test-case\n",
        "4. Conclusion: possible improvements and points to adress\n",
        "\n",
        "5. all codes comes from article below ：\n",
        "\n",
        "  [film-recommendation-engine](https://www.kaggle.com/fabiendaniel/film-recommendation-engine)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae-cXPDyqsUC"
      },
      "source": [
        "#Load package we need\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math, nltk, warnings\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet');nltk.download('averaged_perceptron_tagger')\n",
        "from sklearn import linear_model\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from fuzzywuzzy import fuzz\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
        "plt.style.use('fivethirtyeight')\n",
        "mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"last_expr\"\n",
        "pd.options.display.max_columns = 50\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM0kH0GXrDnq"
      },
      "source": [
        "#link to the google drive we choose\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxizB8A1rlPp"
      },
      "source": [
        "import os\n",
        "main_path = \"/content/drive//MyDrive/adventure_time/movie data\"\n",
        "os.chdir(main_path)\n",
        "os.listdir()#show all data on the direction\n",
        "\n",
        "data = [d for d in os.listdir() if \"tmdb_5000\" in d]\n",
        "\n",
        "#Zip data  \n",
        "import zipfile\n",
        "path_file_to_zip = [os.path.join(main_path,d) for d in data]\n",
        "\n",
        "# for i in path_file_to_zip:\n",
        "#   with zipfile.ZipFile(i,'r') as zip_file:\n",
        "#     print(zip_file.namelist())\n",
        "#     zip_file.extractall(os.getcwd())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qnNbLRz3MBg"
      },
      "source": [
        "# Import and manipulate raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mLYZNy1PuYJ"
      },
      "source": [
        "# Import data\n",
        "file_to_read = [f for f in path_file_to_zip if f.endswith(\".csv\")]\n",
        "\n",
        "df_movie = pd.read_csv(file_to_read[0])\n",
        "df_credit = pd.read_csv(file_to_read[1])\n",
        "\n",
        "#Manipulate data formation：some data are read as json formation.Therefore, we should do some transformation\n",
        "json_columns = ['genres', 'keywords', 'production_countries','production_companies', 'spoken_languages']\n",
        "for i in json_columns:\n",
        "  print(\"column in the mvie data: {}\".format(i))\n",
        "  df_movie[i] = df_movie[i].apply(json.loads)\n",
        "json_columns = ['cast', 'crew']\n",
        "for i in json_columns:\n",
        "  print(\"column in the credit data: \",i)\n",
        "  df_credit[i] = df_credit[i].apply(json.loads)\n",
        "\n",
        "\n",
        "df_initial = convert_to_original_format(df_movie, df_credit)\n",
        "print('Shape:',df_initial.shape)\n",
        "#__________________________________________\n",
        "# info on variable types and filling factor\n",
        "tab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\n",
        "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values'}))\n",
        "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()/df_initial.shape[0]*100).T.\n",
        "                         rename(index={0:'null values (%)'}))\n",
        "print(\"\\nData Info:{}\".format(\"df_initial\"))\n",
        "tab_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tS8AELR6UON"
      },
      "source": [
        "#Combine two data frame to initial df\n",
        "initial_df = convert_to_original_format(df_movie,df_credit)\n",
        "\n",
        "# Viewing data1\n",
        "df_type =initial_df.dtypes.reset_index(name=\"type\").rename(columns={\"index\":\"column\"})\n",
        "df_mis =initial_df.isnull().sum().reset_index(name=\"missing_num\").rename(columns={\"index\":\"column\"})\n",
        "\n",
        "df_str = df_type.join(df_mis.set_index(\"column\"),on=\"column\")\n",
        "df_str['missing_proportion'] = df_str['missing_num']/df_movie.shape[0]*100\n",
        "df_str.sort_values(by=\"missing_proportion\",ascending=False) #We can tell there mamny missing value in the some columns,\n",
        "                               # such as \"homepage\"、\"tagline\"\n",
        "\n",
        "# View data2\n",
        "# df_info = pd.DataFrame(df_initial.dtypes).T.rename(index={0:\"column type\"})\n",
        "# df_info = df_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:\"null number\"}))\n",
        "# df_info = df_info.append(pd.DataFrame(df_initial.isnull().sum()/df_initial.shape[0]*100).T.rename(index = {0:\"null values (%)\"}))\n",
        "# df_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZh0k_c4Fltp"
      },
      "source": [
        "#Create a set of variable in the plot_keywords column\n",
        "set_keywords = set()\n",
        "\n",
        "for i in df_initial.plot_keywords.str.split(\"|\").values:\n",
        "  if isinstance(i,float): continue\n",
        "\n",
        "  set_keywords = set_keywords.union(i)\n",
        "\n",
        "set_keywords.remove(\"\")\n",
        "#Create a dictionary for each variable set with count number of each variable\n",
        "tmp = dict()\n",
        "for i in set_keywords:\n",
        "  tmp[i]=0\n",
        "for i in df_initial['plot_keywords']:\n",
        "  if isinstance(i,float) or pd.isnull(i): continue\n",
        "  for j in [s for s in i if s in set_keywords]:\n",
        "     if pd.notnull(j): tmp[j] += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-KQte2F4uLE"
      },
      "source": [
        "# Manipulate the **Keyword** data in the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLHLpoyROpEt"
      },
      "source": [
        "keyword_count = dict()\n",
        "for s in set_keywords: keyword_count[s] = 0\n",
        "for i in df_initial[\"plot_keywords\"].str.split(\"|\"):\n",
        "  if type(i) == float and pd.isnull(i): continue\n",
        "  for s in [s for s in i if s in set_keywords]:\n",
        "    if pd.notnull(s): keyword_count[s] +=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9O9FhsgM2Ix"
      },
      "source": [
        "#convert dictionary to list\n",
        "keyword_occurences = []# a list of dictionary containing keywords and counts\n",
        "for k,v in keyword_count.items():\n",
        "  keyword_occurences.append([k,v])\n",
        "keyword_occurences.sort(key=lambda x:x[1],reverse=True)\n",
        "\n",
        "print(\"Show the top 5 rows in the dateframe: {}\".format(\"keywords_occurences\"))\n",
        "pd.DataFrame(keyword_occurences,columns=[\"Keyword\",\"count\"]).head()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHJx6NFROc9Y"
      },
      "source": [
        "# wrap upper step into a function\n",
        "\n",
        "def count_word(df, ref_col, liste):\n",
        "    keyword_count = dict()\n",
        "    for s in liste: keyword_count[s] = 0\n",
        "    for liste_keywords in df[ref_col].str.split('|'):        \n",
        "        if type(liste_keywords) == float and pd.isnull(liste_keywords): continue        \n",
        "        for s in [s for s in liste_keywords if s in liste]: \n",
        "            if pd.notnull(s): keyword_count[s] += 1\n",
        "#__________________________________________________________________\n",
        "    # convert the dictionary in a list to sort the keywords by frequency\n",
        "    keyword_occurences = []\n",
        "    for k,v in keyword_count.items():\n",
        "        keyword_occurences.append([k,v])\n",
        "    keyword_occurences.sort(key = lambda x:x[1], reverse = True)\n",
        "    return keyword_occurences, keyword_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWJSfKaJ9HAr"
      },
      "source": [
        "# Go through the keyword procession by function in one line code\n",
        "keyword_occurences, dum = count_word(df_initial, 'plot_keywords', set_keywords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2zCjhUURreJ"
      },
      "source": [
        "#_____________________________________________\n",
        "# Function that control the color of the words\n",
        "# WARNING: the scope of variables is used to get the value of the \"tone\" variable\n",
        "# I could not find the way to pass it as a parameter of \"random_color_func()\"\n",
        "def random_color_func(word=None, font_size=None, position=None,\n",
        "                      orientation=None, font_path=None, random_state=None):\n",
        "    h = int(360.0 * tone / 255.0)\n",
        "    s = int(100.0 * 255.0 / 255.0)\n",
        "    l = int(100.0 * float(random_state.randint(70, 120)) / 255.0)\n",
        "    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n",
        "#_____________________________________________\n",
        "# UPPER PANEL: WORDCLOUD\n",
        "fig = plt.figure(1, figsize=(18,13))\n",
        "ax1 = fig.add_subplot(2,1,1)\n",
        "#_______________________________________________________\n",
        "# I define the dictionary used to produce the wordcloud\n",
        "words = dict()\n",
        "trunc_occurences = keyword_occurences[0:50]\n",
        "for s in trunc_occurences:\n",
        "    words[s[0]] = s[1]\n",
        "tone = 55.0 # define the color of the words\n",
        "#________________________________________________________\n",
        "wordcloud = WordCloud(width=1000,height=300, background_color='black', \n",
        "                      max_words=1628,relative_scaling=1,\n",
        "                      color_func = random_color_func,\n",
        "                      normalize_plurals=False)\n",
        "wordcloud.generate_from_frequencies(words)\n",
        "ax1.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "ax1.axis('off')\n",
        "#_____________________________________________\n",
        "# LOWER PANEL: HISTOGRAMS\n",
        "ax2 = fig.add_subplot(2,1,2)\n",
        "y_axis = [i[1] for i in trunc_occurences]\n",
        "x_axis = [k for k,i in enumerate(trunc_occurences)]\n",
        "x_label = [i[0] for i in trunc_occurences]\n",
        "plt.xticks(rotation=85, fontsize = 15)\n",
        "plt.yticks(fontsize = 15)\n",
        "plt.xticks(x_axis, x_label)\n",
        "plt.ylabel(\"Nb. of occurences\", fontsize = 18, labelpad = 10)\n",
        "ax2.bar(x_axis, y_axis, align = 'center', color='g')\n",
        "#_______________________\n",
        "plt.title(\"Keywords popularity\",bbox={'facecolor':'k', 'pad':5},color='w',fontsize = 25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDCJ3wCE-4jh"
      },
      "source": [
        "# Divide title_year into proper interval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Dw02UaBzxo5"
      },
      "source": [
        "# caculate the decade based on 1900 ：1945 -> 40 decade\n",
        "df_initial['decade'] = df_initial['title_year'].apply(lambda x :((x-1900)//10)*10)\n",
        "\n",
        "\n",
        "def get_stats(gr):\n",
        "  return{'min':gr.min(),\"max\":gr.max(),'count':gr.count(),'mean':gr.mean()}\n",
        "\n",
        "test = df_initial['title_year'].groupby(df_initial['decade']).apply(get_stats)\n",
        "display(pd.DataFrame(test).head(4))\n",
        "test = test.unstack() #pivot a level of the index label(defualt the last level) \n",
        "display(pd.DataFrame(test).head(4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwRR5GlgH-ls"
      },
      "source": [
        "# How does below code work ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG6DCe_r3xAA"
      },
      "source": [
        "def label(s):\n",
        "    val = (1900 + s, s)[s < 100]\n",
        "    chaine = '' if s < 50 else \"{}'s\".format(int(val))\n",
        "    return chaine\n",
        "\n",
        "[label(s) for s in  test.index]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvBmd59Q119r"
      },
      "source": [
        "sns.set_context('poster',font_scale =0.85)\n",
        "\n",
        "def label(s):\n",
        "  val = (s + 1900, s)[s<100]\n",
        "  chaine = '' if s < 50 else \"{}'s\".format(int(val))\n",
        "  return chaine\n",
        "\n",
        "plt.rc('font',weight = 'bold')\n",
        "fig,ax = plt.subplots(figsize=(20,10))\n",
        "\n",
        "labels = [label(s) for s in test.index]\n",
        "sizes = test['count'].values\n",
        "explode = [0.02 if i < 100 else 0.01 for i in range(11)]\n",
        "#crat a pie chart\n",
        "ax.pie(sizes, explode = explode, labels=labels,\n",
        "       autopct = lambda x:'{:1.0f}%'.format(x) if x > 1 else '',\n",
        "       shadow=False, startangle=0)\n",
        "ax.axis('equal')\n",
        "ax.set_title('% of films per decade',bbox={'facecolor':'k','pad':5},color='w',fontsize =16)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTbOTXbhJu-G"
      },
      "source": [
        "# Show the genres distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtLTvVeuJ5Ue"
      },
      "source": [
        "## 1.make a list of all unique genres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UOhfQ9u_Pms"
      },
      "source": [
        "# To see exactly which genres are the most popular, \n",
        "# I use approach as same as handling keywords (hence using similar code), \n",
        "# first making a census of the genres\n",
        "genre_set = set()\n",
        "for i in df_initial['genres'].str.split('|').values:\n",
        "  # if isinstance(i,float):continue\n",
        "  genre_set = genre_set.union(i)\n",
        "  # print(i)\n",
        "  \n",
        "genre_set.remove(\"\")\n",
        "display(genre_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvkYegrhKDlm"
      },
      "source": [
        "## 2.Count the number of each genres in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS0EkpKYkIg9"
      },
      "source": [
        "#Check which genres data point belong to iteratively\n",
        "genres_count = dict()\n",
        "genres_occurences = list() \n",
        "for i in list(genre_set):\n",
        "  genres_count[i] = 0\n",
        "  # print(i)\n",
        "  for j in df_initial['genres'].str.split('|').values:\n",
        "    if i in j: genres_count[i]+=1\n",
        "  genres_occurences.append([i,genres_count[i]])\n",
        "\n",
        "#Sort list by count\n",
        "genres_occurences.sort(key=lambda x:x[1],reverse=True)\n",
        "display(genres_occurences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shVAZUpULZzu"
      },
      "source": [
        "## 3.Define plot function - Word cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQXOvez800s5"
      },
      "source": [
        "def random_color_func(word=None, font_size=None, position=None,\n",
        "                      orientation=None, font_path=None, random_state=None):\n",
        "    h = int(360.0 * tone / 255.0)\n",
        "    s = int(100.0 * 255.0 / 255.0)\n",
        "    l = int(100.0 * float(random_state.randint(70, 120)) / 255.0)\n",
        "    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n",
        "\n",
        "words = dict()\n",
        "trunc_occurences = genres_occurences[0:len(genres_occurences)]\n",
        "for s in trunc_occurences:\n",
        "    words[s[0]] = s[1]\n",
        "tone = 100 # define the color of the words\n",
        "f, ax = plt.subplots(figsize=(14, 6))\n",
        "wordcloud = WordCloud(width=550,height=300, background_color='black', \n",
        "                      max_words=1628,relative_scaling=0.7,\n",
        "                      color_func = random_color_func,\n",
        "                      normalize_plurals=False)\n",
        "wordcloud.generate_from_frequencies(words)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.title(\"The wordcloud of genres\")\n",
        "plt.axis('off')\n",
        "\n",
        "print(\"Plot title: \",\"The wordcloud of genres\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F0Jsnw2eE4Q"
      },
      "source": [
        "df_initial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvXUzYHKajhB"
      },
      "source": [
        "# Update genres column in dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jiqSOeAEPc6"
      },
      "source": [
        "df_initial['genres'] = df_initial['genres'].str.split('|')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEa4_AfcuGFw"
      },
      "source": [
        "# Process keyword column:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45Abi55t6WMN"
      },
      "source": [
        "## 1.Group Keywords by roots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS_LXvfkNO8C"
      },
      "source": [
        "- Use NLTK to do Text Normalization :\n",
        "\n",
        "  use [lemmatizer](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python) to  provide roots of word "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8z3VtbIs3cm"
      },
      "source": [
        "#Copy dataset to prevent so as not to modify the original dataset\n",
        "df_duplicated_cleaned =df_initial.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv4ehHfqtoQN"
      },
      "source": [
        "# Clean the keyword in the plot_keyword by NLTK package\n",
        "lemmatizer = WordNetLemmatizer() #Use lemmatize stemmer has better result than porter stemmer\n",
        "keywords_roots = dict()\n",
        "keywords_select = dict()\n",
        "category_key = []\n",
        "\n",
        "\n",
        "for s in df_duplicated_cleaned['plot_keywords'].str.split(\"|\"):\n",
        "  if all(s) == False:continue\n",
        "  tmp = [ss.lower() for ss in s]\n",
        "  for sss in tmp:\n",
        "    racine = lemmatizer.lemmatize(sss)\n",
        "    if  racine in keywords_roots:\n",
        "      keywords_roots[racine].add(sss)\n",
        "\n",
        "    else:\n",
        "      keywords_roots[racine] = {sss}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaASyDH4y04A"
      },
      "source": [
        "#Replace plot_keywords by roots\n",
        "df_duplicated_cleaned['plot_keywords']=df_duplicated_cleaned['plot_keywords'].str.split(\"|\").apply(lambda x:[lemmatizer.lemmatize(s) for s in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B7-fO9-qT9I"
      },
      "source": [
        "def word_count(df,col):\n",
        "  keywords_dict = dict()\n",
        "  keywords_list = list()\n",
        "  for i in df[col]:\n",
        "    if any(i):\n",
        "      for s in i:\n",
        "        if s in keywords_dict:\n",
        "          keywords_dict[s] += 1\n",
        "        else:\n",
        "          keywords_dict[s] = 1\n",
        "  \n",
        "  for k,val in keywords_dict.items():\n",
        "    keywords_list.append([k,val])\n",
        "  keywords_list.sort(key=lambda x:x[1],reverse=True)\n",
        "  return keywords_dict,keywords_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw1ZAdqDzNdj"
      },
      "source": [
        "keywords_roots,keywords_counts = word_count(df_duplicated_cleaned,'plot_keywords')\n",
        "\n",
        "print(\"the total amount of various keyword in the plot_keywords: {}\".format(len(keywords_roots)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoOUfGv6ucl-"
      },
      "source": [
        "##2.Group Keywords by synonyms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JBnC66u9ata"
      },
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# get the synomyms of the word \n",
        "#--------------------------------------------------------------\n",
        "def get_synonymes(word):\n",
        "  #get the synonyme of word\n",
        "  #put them into set of data\n",
        "  lemma = set()\n",
        "  if any(wn.synsets(word,pos = wn.NOUN)):\n",
        "    for i in wn.synsets(word,pos = wn.NOUN):\n",
        "      for s in i.lemma_names():\n",
        "        lemma.add(s)\n",
        "  \n",
        "  return lemma\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------\n",
        "def detector(word):\n",
        "  # detect whether word has sysnonymes or not\n",
        "    lemma = get_synonymes(word)\n",
        "    if any(lemma):\n",
        "      #make a list to provide the synonymes and their count in a list if they exist\n",
        "      #sort the list by count and letter in a in descending order\n",
        "      result = [(s , keywords_roots[s]) for s in lemma if s in keywords_roots]\n",
        "      result.sort(key=lambda x:(x[1],x[0]),reverse=True)\n",
        "      return result if any(result) else \"no synonyme\"\n",
        "    else:\n",
        "      return \"no synonyme\"      \n",
        "                                                          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWE3RL46A83U"
      },
      "source": [
        " replacement_mot = dict()\n",
        " n=0\n",
        " for i in keywords_roots.keys():\n",
        "   if detector(i) != \"no synonyme\":\n",
        "      if i != detector(i)[0][0]:\n",
        "        #if word in the keyword_root isn't equal to the sysnonym\n",
        "        #put the synonyme into replace list\n",
        "        replacement_mot[i] = detector(i)[0][0]\n",
        "        n+=1\n",
        "\n",
        "print(\"How many words should been replace by synonyme: {}\".format(n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHbrteIaCQJB"
      },
      "source": [
        "# replace word with synonyme\n",
        "n=0\n",
        "for i in df_duplicated_cleaned['plot_keywords']:\n",
        "  df_duplicated_cleaned['plot_keywords'][n] = [replacement_mot[s] if s in replacement_mot else s for s in i]\n",
        "  n+=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AozmJuO-2MFv"
      },
      "source": [
        "keywords_roots,keywords_counts  = word_count(df_duplicated_cleaned,'plot_keywords')\n",
        "print(\"the total amount of various keyword in the plot_keywords: {}\".format(len(keywords_roots)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJUwAM-M2z2w"
      },
      "source": [
        "above_threshold = list()\n",
        "for k,val in keywords_roots.items():\n",
        "  if val > 3:\n",
        "    above_threshold.append(k)\n",
        "\n",
        "print(\"number of keywords above threshold:\",len(above_threshold))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXhbdQYZ1Cpz"
      },
      "source": [
        "#replace with synonymes which freauency are above 3\n",
        "tmp = list()\n",
        "n = 0\n",
        "for i in df_duplicated_cleaned['plot_keywords']:\n",
        "  ls = [s  for s in i if s in above_threshold]\n",
        "  tmp.append(ls)\n",
        "\n",
        "\n",
        "df_duplicated_cleaned['plot_keywords'] = tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuCzYdoa454L"
      },
      "source": [
        "#create a list of synonyme and it count in the data \n",
        "dic = dict()\n",
        "for i in tmp:\n",
        "  for s in i:\n",
        "    if s in dic:\n",
        "      dic[s] += 1\n",
        "    else:\n",
        "      dic[s] = 0\n",
        "\n",
        "ls = list()\n",
        "for k,val in dic.items():\n",
        "  ls.append([k,val])\n",
        "\n",
        "ls.sort(key=lambda x: x[1],reverse = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82ANPI-Z3sOJ"
      },
      "source": [
        "# Graph of keyword occurences\n",
        "#----------------------------\n",
        "font = {'family' : 'fantasy', 'weight' : 'normal', 'size'   : 15}\n",
        "mpl.rc('font', **font)\n",
        "\n",
        "keywords_counts.sort(key = lambda x:x[1], reverse = True)\n",
        "\n",
        "y_axis = [i[1] for i in keywords_counts]\n",
        "x_axis = [k for k,i in enumerate(keywords_counts)]\n",
        "\n",
        "new_y_axis = [i[1] for i in ls]\n",
        "new_x_axis = [k for k,i in enumerate(ls)]\n",
        "\n",
        "f, ax = plt.subplots(figsize=(9, 5))\n",
        "ax.plot(x_axis, y_axis, 'r-', label='before cleaning')\n",
        "ax.plot(new_x_axis, new_y_axis, 'b-', label='after cleaning')\n",
        "\n",
        "# Now add the legend with some customizations.\n",
        "legend = ax.legend(loc='upper right', shadow=True)\n",
        "frame = legend.get_frame()\n",
        "frame.set_facecolor('0.90')\n",
        "for label in legend.get_texts():\n",
        "    label.set_fontsize('medium')\n",
        "            \n",
        "plt.ylim((0,25))\n",
        "plt.axhline(y=3.5, linewidth=2, color = 'k')\n",
        "plt.xlabel(\"keywords index\", family='fantasy', fontsize = 15)\n",
        "plt.ylabel(\"Nb. of occurences\", family='fantasy', fontsize = 15)\n",
        "plt.text(3500, 4.5, 'threshold for keyword delation', fontsize = 13)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BdHedlLSIF-"
      },
      "source": [
        "# Plot correlation matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2WobvjtNdth"
      },
      "source": [
        "#caculate new keywords_roots and keyword_count after data cleaning\n",
        "keywords_roots, keyword_count = word_count(df_duplicated_cleaned,\"plot_keywords\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qegat49vPsNd"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(12,9))\n",
        "\n",
        "#caculate correaltion matrix\n",
        "#__________________________________________________________\n",
        "corrmat = df_duplicated_cleaned.corr(method ='pearson')\n",
        "\n",
        "#Order the index by correlation with num_voted_users\n",
        "#nlargest: return the first n rows vlaues orderd by given columns in decending order\n",
        "#return the cols in the order from most to least related to num_voted_users\n",
        "#we can tell from result: except the num_voted_users itself,the most related col is gross; \n",
        "#the least related col is title_year\n",
        "#__________________________________________________________\n",
        "cols = corrmat.nlargest(17,'num_voted_users')['num_voted_users'].index\n",
        "\n",
        "#Get the correlation coffience\n",
        "#__________________________________________________________\n",
        "cm = np.corrcoef(df_duplicated_cleaned[cols].dropna(how='any').values.T)\n",
        "\n",
        "sns.set(font_scale=1.25)\n",
        "hm = sns.heatmap(cm, cbar=True, annot=True, square=True,\n",
        "                 fmt='.2f', annot_kws={'size': 15}, linewidth = 0.1, cmap = 'coolwarm',\n",
        "                 yticklabels=cols.values, xticklabels=cols.values)\n",
        "\n",
        "fig.text(0.5, 0.93, \"Correlation coefficients\", ha='center', fontsize = 18, family='fantasy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW6TOczjUrq-"
      },
      "source": [
        "# Handling Missing value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn5GCXBzWKdp"
      },
      "source": [
        "## Inspect the missing value proportion of each column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7fnXJEcTNtK"
      },
      "source": [
        "missing_value_df = df_duplicated_cleaned.isnull().sum().reset_index()\n",
        "missing_value_df.columns = ['columns',\"num_missing_value\"]\n",
        "missing_value_df['filling_ratio'] = (1-(missing_value_df.num_missing_value/df_duplicated_cleaned.shape[0]))*100\n",
        "#Reset the index, or a level of it.\n",
        "missing_value_df = missing_value_df.sort_values(by = \"filling_ratio\").reset_index().drop([\"index\"],axis =1)\n",
        "missing_value_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAcWC6eiYRX5"
      },
      "source": [
        "y_axis = missing_value_df[\"filling_ratio\"]\n",
        "x_label = missing_value_df['columns']\n",
        "x_axis = missing_value_df.index\n",
        "\n",
        "fig = plt.figure(figsize=(11,4))\n",
        "plt.xticks(rotation = 80,fontsize=14)\n",
        "plt.yticks(fontsize=13)\n",
        "\n",
        "plt.xticks(x_axis,x_label,family='fantasy',fontsize=14)\n",
        "plt.ylabel('Filling ratio (%)', family='fantasy', fontsize = 16)\n",
        "plt.bar(x_axis, y_axis);\n",
        "plt.title(\"Filling ratio of reach columns\",family=\"fantasy\",fontsize=20)\n",
        "\n",
        "#Draw a ceritcal lines\n",
        "N_threshold =5\n",
        "plt.axvline(x=N_threshold+0.5,linewidth=2,color='r')\n",
        "N_thresh = 17\n",
        "plt.axvline(x=N_thresh-0.5, linewidth=2, color = 'g')\n",
        "\n",
        "#Add a text box which divide bar plot into three segment\n",
        "#Text box indicate the threshold ratio of each segment\n",
        "plt.text(N_threshold-4.5, 30,\"filling ratio \\n < {}%\".format(round(y_axis[N_threshold],1)),\n",
        "         fontsize = 15, family = 'fantasy', bbox=dict(boxstyle=\"round\",\n",
        "                                                      ec=(1.0, 0.5, 0.5),\n",
        "                                                      fc=(0.8, 0.5, 0.5)))\n",
        "\n",
        "plt.text(N_thresh, 30, 'filling factor \\n = {}%'.format(round(y_axis[N_thresh],1)),\n",
        "         fontsize = 15, family = 'fantasy', bbox=dict(boxstyle=\"round\",\n",
        "                                                      ec=(1., 0.5, 0.5),\n",
        "                                                      fc=(0.5, 0.8, 0.5)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-GDb4JXulH4"
      },
      "source": [
        "df_duplicated_missing_cleaned = df_duplicated_cleaned.copy()\n",
        "df_duplicated_missing_cleaned = df_duplicated_missing_cleaned.dropna()\n",
        "df_duplicated_missing_cleaned = df_duplicated_missing_cleaned.isnull().sum().reset_index()\n",
        "df_duplicated_missing_cleaned.rename(columns={\"index\":'column',0:\"missing value\"},inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3YKIGyUpy4e"
      },
      "source": [
        "df_duplicated_cleaned[df_duplicated_cleaned['homepage'].isnull() ==False]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj32QfQZi4Hk"
      },
      "source": [
        "# Setting missing title years"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4q7E7KGpwZR"
      },
      "source": [
        " def fill_year(df):\n",
        "    col = ['director_name', 'actor_1_name', 'actor_2_name', 'actor_3_name']\n",
        "    usual_year = [0 for _ in range(4)]\n",
        "    var        = [0 for _ in range(4)]\n",
        "    #_____________________________________________________________\n",
        "    # I get the mean years of activity for the actors and director\n",
        "    for i in range(4):\n",
        "        usual_year[i] = df.groupby(col[i])['title_year'].mean()\n",
        "    #_____________________________________________\n",
        "    # I create a dictionnary collectinf this info\n",
        "    actor_year = dict()\n",
        "    for i in range(4):\n",
        "        for s in usual_year[i].index:\n",
        "            if s in actor_year.keys():\n",
        "                if pd.notnull(usual_year[i][s]) and pd.notnull(actor_year[s]):\n",
        "                    actor_year[s] = (actor_year[s] + usual_year[i][s])/2\n",
        "                elif pd.isnull(actor_year[s]):\n",
        "                    actor_year[s] = usual_year[i][s]\n",
        "            else:\n",
        "                actor_year[s] = usual_year[i][s]\n",
        "        \n",
        "    #______________________________________\n",
        "    # identification of missing title years\n",
        "    missing_year_info = df[df['title_year'].isnull()]\n",
        "    #___________________________\n",
        "    # filling of missing values\n",
        "    icount_replaced = 0\n",
        "    for index, row in missing_year_info.iterrows():\n",
        "        value = [ np.NaN for _ in range(4)]\n",
        "        icount = 0 ; sum_year = 0\n",
        "        for i in range(4):            \n",
        "            var[i] = df.loc[index][col[i]]\n",
        "            if pd.notnull(var[i]): value[i] = actor_year[var[i]]\n",
        "            if pd.notnull(value[i]): icount += 1 ; sum_year += actor_year[var[i]]\n",
        "        if icount != 0: sum_year = sum_year / icount \n",
        "\n",
        "        if int(sum_year) > 0:\n",
        "            icount_replaced += 1\n",
        "            df.set_value(index, 'title_year', int(sum_year))\n",
        "            if icount_replaced < 10: \n",
        "                print(\"{:<45} -> {:<20}\".format(df.loc[index]['movie_title'],int(sum_year)))\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XAye4chtCaV"
      },
      "source": [
        "df_filling = df_duplicated_cleaned.copy(deep=True)\n",
        "missing_year_info = df_filling[df_filling['title_year'].isnull()][[\n",
        "            'director_name','actor_1_name', 'actor_2_name', 'actor_3_name']]\n",
        "missing_year_info[:10]\n",
        "fill_year(df_filling)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTixXFn5KVYM"
      },
      "source": [
        "def fill_year(df):\n",
        "    col = ['director_name', 'actor_1_name', 'actor_2_name', 'actor_3_name']\n",
        "    usual_year = [0 for _ in range(4)]\n",
        "    var        = [0 for _ in range(4)]\n",
        "    #_____________________________________________________________\n",
        "    # I get the mean years of activity for the actors and director\n",
        "    for i in range(4):\n",
        "        usual_year[i] = df.groupby(col[i])['title_year'].mean()\n",
        "    #_____________________________________________\n",
        "    # I create a dictionnary collectinf this info\n",
        "    actor_year = dict()\n",
        "    for i in range(4):\n",
        "        for s in usual_year[i].index:\n",
        "            if s in actor_year.keys():\n",
        "                if pd.notnull(usual_year[i][s]) and pd.notnull(actor_year[s]):\n",
        "                    actor_year[s] = (actor_year[s] + usual_year[i][s])/2\n",
        "                elif pd.isnull(actor_year[s]):\n",
        "                    actor_year[s] = usual_year[i][s]\n",
        "            else:\n",
        "                actor_year[s] = usual_year[i][s]\n",
        "        \n",
        "    #______________________________________\n",
        "    # identification of missing title years\n",
        "    missing_year_info = df[df['title_year'].isnull()]\n",
        "    #___________________________\n",
        "    # filling of missing values\n",
        "    icount_replaced = 0\n",
        "    for index, row in missing_year_info.iterrows():\n",
        "        value = [ np.NaN for _ in range(4)]\n",
        "        icount = 0 ; sum_year = 0\n",
        "        for i in range(4):            \n",
        "            var[i] = df.loc[index][col[i]]\n",
        "            if pd.notnull(var[i]): value[i] = actor_year[var[i]]\n",
        "            if pd.notnull(value[i]): icount += 1 ; sum_year += actor_year[var[i]]\n",
        "        if icount != 0: sum_year = sum_year / icount \n",
        "\n",
        "        if int(sum_year) > 0:\n",
        "            icount_replaced += 1\n",
        "            df.set_value(index, 'title_year', int(sum_year))\n",
        "            if icount_replaced < 10: \n",
        "                print(\"{:<45} -> {:<20}\".format(df.loc[index]['movie_title'],int(sum_year)))\n",
        "    return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI1UKCOjKWX-"
      },
      "source": [
        "fill_year(df_filling)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uan-yNi_KY8a"
      },
      "source": [
        "icount = 0\n",
        "for index, row in df_filling[df_filling['plot_keywords'].isnull()].iterrows():\n",
        "    icount += 1\n",
        "    liste_mot = row['movie_title'].strip().split()\n",
        "    new_keyword = []\n",
        "    for s in liste_mot:\n",
        "        lemma = get_synonymes(s)\n",
        "        for t in list(lemma):\n",
        "            if t in keywords: \n",
        "                new_keyword.append(t)                \n",
        "    if new_keyword and icount < 15: \n",
        "        print('{:<50} -> {:<30}'.format(row['movie_title'], str(new_keyword)))\n",
        "    if new_keyword:\n",
        "        df_filling.set_value(index, 'plot_keywords', '|'.join(new_keyword)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfXZ4i8EKfM6"
      },
      "source": [
        "cols = corrmat.nlargest(9, 'num_voted_users')['num_voted_users'].index\n",
        "cm = np.corrcoef(df_duplicated_cleaned[cols].dropna(how='any').values.T)\n",
        "sns.set(font_scale=1.25)\n",
        "hm = sns.heatmap(cm, cbar=True, annot=True, square=True,\n",
        "                 fmt='.2f', annot_kws={'size': 10}, \n",
        "                 yticklabels=cols.values, xticklabels=cols.values)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id_8CUr0KiGa"
      },
      "source": [
        "sns.set(font_scale=1.25)\n",
        "cols = ['gross', 'num_voted_users']\n",
        "sns.pairplot(df_filling.dropna(how='any')[cols],diag_kind='kde', size = 2.5)\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsj1gw8pdtRK"
      },
      "source": [
        "cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do6Sv8gkKlhn"
      },
      "source": [
        "def variable_linreg_imputation(df, col_to_predict, ref_col):\n",
        "    regr = linear_model.LinearRegression()\n",
        "    test = df[[col_to_predict,ref_col]].dropna(how='any', axis = 0)\n",
        "    X = np.array(test[ref_col])\n",
        "    Y = np.array(test[col_to_predict])\n",
        "    X = X.reshape(len(X),1)\n",
        "    Y = Y.reshape(len(Y),1)\n",
        "    regr.fit(X, Y)\n",
        "    \n",
        "    test = df[df[col_to_predict].isnull() & df[ref_col].notnull()]\n",
        "    for index, row in test.iterrows():\n",
        "        value = float(regr.predict(row[ref_col]))\n",
        "        df.set_value(index, col_to_predict, value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJpOwevMKoDO"
      },
      "source": [
        "variable_linreg_imputation(df_filling, 'gross', 'num_voted_users')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osjYJmLmKq3z"
      },
      "source": [
        "df = df_filling.copy(deep = True)\n",
        "missing_df = df.isnull().sum(axis=0).reset_index()\n",
        "missing_df.columns = ['column_name', 'missing_count']\n",
        "missing_df['filling_factor'] = (df.shape[0] \n",
        "                                - missing_df['missing_count']) / df.shape[0] * 100\n",
        "missing_df = missing_df.sort_values('filling_factor').reset_index(drop = True)\n",
        "missing_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mve5OV-pKrk8"
      },
      "source": [
        "df = df_filling.copy(deep=True)\n",
        "df.reset_index(inplace = True, drop = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0sL2EXXh3Il"
      },
      "source": [
        " # Extracting keywords from the title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXxSshc0jNwz"
      },
      "source": [
        "# Imputing from regressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHSSXUl8jRHP"
      },
      "source": [
        "# RECOMMENDATION ENGINE\n",
        "---\n",
        "We build recommendation Engine in two step:\n",
        "1. Determine N films with content simularity \n",
        "2. choose top 5 popular films to recommend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D6yqszSjXyW"
      },
      "source": [
        "## Determine N films with content similarity \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpz3V8hfks4_"
      },
      "source": [
        "gaussian_filter = lambda x,y,sigma: math.exp(-(x-y)**2/(2*sigma**2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAHSN91up5aL"
      },
      "source": [
        "def entry_variables(df,id_entry):\n",
        "  col_labels = []\n",
        "  #add director_name to the list\n",
        "  if pd.notnull(df['director_name'].iloc[id_entry]):\n",
        "    col_labels.append(df['director_name'].iloc[id_entry])\n",
        "\n",
        "  # add actor N name to the list\n",
        "  act_nm = df[['actor_1_name','actor_2_name','actor_3_name']].iloc[id_entry].tolist()\n",
        "  [col_labels.append(nm) for nm in act_nm if pd.notnull(nm)]\n",
        "  \n",
        "\n",
        "  #add keyword to the list\n",
        "  [col_labels.append(key) for key in df['plot_keywords'].iloc[id_entry]]\n",
        "\n",
        "  return col_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGTy7mmGPLkn"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2Mo4xPd-47M"
      },
      "source": [
        "def adjust_format(df,idx_col,nm_col):\n",
        "  tmp = df.set_index(idx_col)[nm_col].explode().reset_index()\n",
        "  tmp = pd.get_dummies(tmp,columns=[nm_col],prefix=\"\",prefix_sep=\"\").groupby(idx_col).sum()\n",
        "  return tmp\n",
        "  \n",
        "df['all_staff']=df[['director_name','actor_1_name','actor_2_name','actor_3_name']].values.tolist()\n",
        "genre_list = adjust_format(df,'movie_title','genres')\n",
        "keyword_list = adjust_format(df,'movie_title','plot_keywords')\n",
        "staff_list = adjust_format(df,'movie_title','all_staff')\n",
        "# staff_list = df[[\"movie_title\",'director_name','actor_1_name','actor_2_name','actor_3_name']].set_index('movie_title')\n",
        "# staff_list = pd.get_dummies(staff_list,columns=['director_name','actor_1_name','actor_2_name','actor_3_name'],prefix_sep=\"\",prefix=\"\").groupby(\"movie_title\").sum()\n",
        "\n",
        "all_list = staff_list.join(genre_list).join(keyword_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmnroFoajwVi"
      },
      "source": [
        "# [Document of K-NearestNeighbor](https://scikit-learn.org/stable/modules/neighbors.html#unsupervised-neighbors)\n",
        "---\n",
        "- abstract：\n",
        "  1. Unsupervised learner of implemeting nearest neighbor searching\n",
        "  2. dist：distance of the k neighbors to each point(sort by distance in descending order)\n",
        "  3. indices：the index of the k neighbors to each point(sort by distance in descending order)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfefoQCml4K_"
      },
      "source": [
        "display(chosen_movie)\n",
        "recommedation_list = all_list.iloc[indices[chosen_movie]].index.tolist()\n",
        "movies_recommedation = df.loc[df['movie_title'].isin(recommedation_list),['movie_title', 'vote_average','num_voted_users', 'title_year']]\n",
        "# all_list.iloc[[chosen_movie]]\n",
        "# indices[chosen_movie]\n",
        "# df.iloc[indices[chosen_movie]][['movie_title', 'vote_average','num_voted_users', 'title_year']]\n",
        "# df.iloc[df['movie_title'].isin(recommedation_list),]\n",
        "# # movies_recommedation\n",
        "movies_recommedation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx-erc45OsF_"
      },
      "source": [
        "var = entry_variables(df,2)\n",
        "X = all_list[var].to_numpy()\n",
        "nbrs = NearestNeighbors(n_neighbors=31,algorithm='auto',metric='euclidean').fit(X)\n",
        "\n",
        "dist, indices = nbrs.kneighbors(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gbSngiT6reV"
      },
      "source": [
        "def critere_selection(title_main, max_users, year1, year2, imdb_score, votes):    \n",
        "    if pd.notnull(year1):\n",
        "        facteur_1 = gaussian_filter(year1, year2, 20)\n",
        "    else:\n",
        "        facteur_1 = 1        \n",
        "\n",
        "    sigma = max_users * 1.0\n",
        "\n",
        "    if pd.notnull(votes):\n",
        "        facteur_2 = gaussian_filter(votes, max_users, sigma)\n",
        "    else:\n",
        "        facteur_2 = 0\n",
        "        \n",
        "    note = imdb_score**2 * facteur_1 * facteur_2\n",
        "    \n",
        "    return note\n",
        "\n",
        "chosen_movie =df[\"movie_title\"].iloc[2]\n",
        "chosen_movie =all_list.index.tolist().index(chosen_movie)\n",
        "recommedation_list = all_list.iloc[indices[chosen_movie]].index.tolist()\n",
        "movies_recommedation = df.loc[df['movie_title'].isin(recommedation_list),['movie_title', 'vote_average','num_voted_users', 'title_year']]\n",
        "max_users = max(movies_recommedation[\"num_voted_users\"])\n",
        "year = movies_recommedation[\"title_year\"].iloc[0]\n",
        "\n",
        "\n",
        "sorted_list = pd.DataFrame()\n",
        "for i in range(len(movies_recommedation)):\n",
        "  temp_df=pd.DataFrame()\n",
        "  name = movies_recommedation.iloc[i]['movie_title']\n",
        "  note = critere_selection(movies_recommedation.iloc[i]['movie_title'],\n",
        "                max_users,\n",
        "                year,\n",
        "                movies_recommedation.iloc[i]['title_year'],\n",
        "                movies_recommedation.iloc[i]['vote_average'],\n",
        "                movies_recommedation.iloc[i]['num_voted_users'])\n",
        "  \n",
        "\n",
        "  temp_df[\"movie_title\"] = [name]\n",
        "  temp_df['popularity'] = [note]\n",
        "  temp_df['simularity'] = [dist[chosen_movie][i]]\n",
        "  print(temp_df)\n",
        "  sorted_list=pd.concat([sorted_list,temp_df],axis=0)\n",
        "\n",
        "\n",
        "  \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQLvYssTLl1T"
      },
      "source": [
        "test1 = [(idx,val) for idx,val in sorted_list.items()]\n",
        "test1.sort(key=lambda x:x[1],reverse=True)\n",
        "# sorted_list.values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9Xdq0guOaxT"
      },
      "source": [
        "sorted_list.sort_values(by=\"popularity\",ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzDwvK1jHmKF"
      },
      "source": [
        "# df.iloc[[12]]['movie_title']\n",
        "df.iloc[12]['num_voted_users']\n",
        "# df.iloc[[12]]['title_year']\n",
        "# df.iloc[[12]]['title_year']\n",
        "# df.iloc[[12]['vte_average']]\n",
        "# df.iloc[[12]]['num_voted_users']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iic3byLZHBK7"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sql-VE5GkhPS"
      },
      "source": [
        "# all_list[var].to_numpy()[0][all_list[var].to_numpy()[0]>0]\n",
        "# all_list[var][all_list[var] > 0]\n",
        "tmp =all_list.reset_index()\n",
        "tmp[tmp.movie_title==\"Pirates of the Caribbean: Dead Man's Chest\"]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE102RS16Ziw"
      },
      "source": [
        "var = entry_variables(df,12)\n",
        "var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vyKG84f_tRg"
      },
      "source": [
        "def add_variables(df, REF_VAR):    \n",
        "    for s in REF_VAR: df[s] = pd.Series([0 for _ in range(len(df))])\n",
        "    colonnes = ['genres', 'actor_1_name', 'actor_2_name',\n",
        "                'actor_3_name', 'director_name', 'plot_keywords']\n",
        "    for categorie in colonnes:\n",
        "        for index, row in df.iterrows():\n",
        "            if pd.isnull(row[categorie]): continue\n",
        "            for s in row[categorie].split('|'):\n",
        "                if s in REF_VAR: df.at[index,s]= 1            \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjtaVqO-w3q6"
      },
      "source": [
        "#Below are code testing area\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkeN_zTUw6JA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myMawK2Rw5k8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHda-tpO2IIX"
      },
      "source": [
        "TMDB_TO_IMDB_SIMPLE_EQUIVALENCIES = {\n",
        "    'budget': 'budget',\n",
        "    'genres': 'genres',\n",
        "    'revenue': 'gross',\n",
        "    'title': 'movie_title',\n",
        "    'runtime': 'duration',\n",
        "    'original_language': 'language',\n",
        "    'keywords': 'plot_keywords',\n",
        "    'vote_count': 'num_voted_users'}\n",
        "df_movie.rename(columns=TMDB_TO_IMDB_SIMPLE_EQUIVALENCIES,inplace=True)\n",
        "pd.to_datetime(df_movie.release_date).apply(lambda x:int(x.strftime(\"%Y\")) if pd.notnull(x) else x)\n",
        "\n",
        "tmdb_movies['country'] = tmdb_movies['production_countries'].apply(lambda x: safe_access(x, [0, 'name']))\n",
        "df_movie.production_countries\n",
        "# display(df_movie.rename(columns=TMDB_TO_IMDB_SIMPLE_EQUIVALENCIES).columns)\n",
        "# display(df_movie.columns)\n",
        "\n",
        "\n",
        "type(pd.to_datetime(df_movie.release_date)[0].year)\n",
        "\n",
        "# display(type(df_movie.release_date[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJwrGS2c9-PY"
      },
      "source": [
        "# type(df_movie.production_countries[0][0])\n",
        "\n",
        "df_movie.production_countries.apply(lambda x: safe_access(x, [0, 'name']))\n",
        "# pd.to_datetime(df_movie.release_date).apply(lambda x:int(x.year),)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edg1cpw-Alwh"
      },
      "source": [
        "for i in [\"name\"]:\n",
        "  print(df_movie.iloc[0][\"production_countries\"][i])\n",
        "\n",
        "# df_movie.iloc[0][\"production_countries\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D460mfGZdgs"
      },
      "source": [
        "# display(df_credit['cast'])\n",
        "df_credit = pd.read_csv(file_to_read[0])\n",
        "# display(df_credit['cast'])\n",
        "display(df_credit['cast'][0])\n",
        "display(df_credit['cast'].apply(json.loads)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pciG64jsR1Uj"
      },
      "source": [
        "# Viewing data\n",
        "df_type =df_movie.dtypes.reset_index(name=\"type\").rename(columns={\"index\":\"column\"})\n",
        "df_mis =df_movie.isnull().sum().reset_index(name=\"missing_num\").rename(columns={\"index\":\"column\"})\n",
        "\n",
        "df_str = df_type.join(df_mis.set_index(\"column\"),on=\"column\")\n",
        "df_str['missing_proportion'] = df_str['missing_num']/df_movie.shape[0]*100\n",
        "df_str.sort_values(by=\"missing_proportion\",ascending=False) #We can tell there mamny missing value in the some columns,\n",
        "                               # such as \"homepage\"、\"tagline\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvbK5IF-7Eog"
      },
      "source": [
        "set_keyword = dict()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh2M-Fzit7TP"
      },
      "source": [
        "# Don't Know why the  bar plot doesn't work\n",
        "trunc_occurences = genres_occurences[0:len(genres_occurences)]\n",
        "\n",
        "#_____________________________________________\n",
        "# LOWER PANEL: HISTOGRAMS\n",
        "ax2 = fig.add_subplot(2,1,2)\n",
        "y_axis = [i[1] for i in trunc_occurences]\n",
        "x_axis = [k for k,i in enumerate(trunc_occurences)]\n",
        "x_label = [i[0] for i in trunc_occurences]\n",
        "\n",
        "\n",
        "# x_axis\n",
        "# x_label\n",
        "plt.xticks(rotation=85, fontsize = 15)\n",
        "plt.yticks(fontsize = 15)\n",
        "plt.xticks(x_axis, x_label)\n",
        "plt.ylabel(\"Nb. of occurences\", fontsize = 18, labelpad = 10)\n",
        "ax2.bar(x_axis, y_axis, align = 'center', color='g')\n",
        "# #_______________________\n",
        "# plt.title(\"Keywords popularity\",bbox={'facecolor':'k', 'pad':5},color='w',fontsize = 25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}