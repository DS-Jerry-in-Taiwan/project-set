{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep learning - audio processing.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOpy53hkzHSEn7xCdi0WK/a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jerremiah/project-set/blob/main/Deep_learning_audio_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD6QwIpDWn3L"
      },
      "source": [
        "# How to make array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdlNRGx11fTb"
      },
      "source": [
        "import numpy as np\n",
        "A = np.array([(56,0,4.4,68),(1.2,104,52,8),(1.8,135,99,0.9)])\n",
        "\n",
        "cal = A.sum(axis=0)\n",
        "\n",
        "p=100*A/cal.reshape(1,4)\n",
        "print(p)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hSwCqQAZEbX"
      },
      "source": [
        "# What is Neural Network?\n",
        "- It's a subset of Machine learning field\n",
        "- Containing input,weight,bias\n",
        "- caculation with non-linear activation function\n",
        "- article：[what is nueral network](https://www.ibm.com/cloud/learn/neural-networks)、[AI vs. Machine Learning vs. Deep Learning vs. Neural Networks: What’s the Difference?](https://www.ibm.com/cloud/blog/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks)、[A Beginner's Guide to Neural Networks and Deep Learning](https://wiki.pathmind.com/neural-network)、[What Is a Neural Network? An Introduction with Examples\n",
        "](https://www.bmc.com/blogs/neural-network-introduction/)、[Machine Learning for Beginners: An Introduction to Neural Networks](https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9)、[機器學習技法 學習筆記 (6)：神經網路(Neural Network)與深度學習(Deep Learning)](https://www.ycc.idv.tw/ml-course-techniques_6.html)、[Neural Network 的概念探討](https://ithelp.ithome.com.tw/articles/10191528)\n",
        "- Book：Deep Learning with Python by Francois Chollet\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "210D3YDEW3pK"
      },
      "source": [
        "# Make a nerual network from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFaZ4xVk3I8t"
      },
      "source": [
        "# simulation how the neural work\n",
        "import math\n",
        "\n",
        "def sigmoid(x):\n",
        "  y = 1/(1+math.exp(-x))\n",
        "  return y\n",
        "\n",
        "def active_fun(inputs,weights):\n",
        "  h=0\n",
        "  for w,i in zip(inputs,weights):\n",
        "    h += w*i\n",
        "  return sigmoid(h)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  inputs = [.5,.3,.2]\n",
        "  weights = [.4,.7,.2]\n",
        "  output = active_fun(inputs,weights)\n",
        "  print(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFkUP9UYY9ZI"
      },
      "source": [
        "# What is multiple layer perceptron\n",
        "- Use more than two layers to cacualte(input,output)\n",
        "- Give input with default weight to linear function.\n",
        "- Put linear function into non-linear activation function through hidden layer\n",
        "- Put the output of hidden layer into activation function in output layer and get the final value\n",
        "- article：[How Do Activation Functions Introduce Non-Linearity In Neural Networks?](https://analyticsindiamag.com/how-do-activation-functions-introduce-non-linearity-in-neural-networks/)、[How to Choose an Activation Function for Deep Learning](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/)、[深度學習：使用激勵函數的目的、如何選擇激勵函數 Deep Learning : the role of the activation function](https://mropengate.blogspot.com/2017/02/deep-learning-role-of-activation.htm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwMwPPRAimGy"
      },
      "source": [
        "# Demostrate the Mpl-building process with forward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-Xi5FTge93z"
      },
      "source": [
        "# How to code a simple example of multiple layer perceptron(MLP) from scratch\n",
        "#first definite a class od MLP\n",
        "\n",
        "import numpy as np\n",
        "class MLP:\n",
        "  def __init__(self, num_inputs=3 , num_hiddens=[3,5] , num_outputs=2):\n",
        "    self.num_inputs = num_inputs\n",
        "    self.num_hiddens = num_hiddens\n",
        "    self.num_outputs = num_outputs\n",
        "    self.weights = list()\n",
        "    layers = [self.num_inputs] + self.num_hiddens + [self.num_outputs]\n",
        "    #Create initial random weights matrix\n",
        "    for i in range(len(layers)-1):\n",
        "      w = np.random.rand(layers[i],layers[i+1])\n",
        "      self.weights.append(w)\n",
        "\n",
        "  #Using non-linear activation function\n",
        "  def sigmoid(self,x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "  def forwrad_propagate(self,inputs):\n",
        "    activate = inputs\n",
        "    #caculate net inputs\n",
        "    for w in self.weights:\n",
        "      net_inputs = np.dot(activate,w)\n",
        "\n",
        "    #caculate activate\n",
        "      activate = self.sigmoid(net_inputs)\n",
        "\n",
        "    return activate\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  #Create a mlp\n",
        "  mlp = MLP()\n",
        "  #Create some inputs\n",
        "  inputs = np.random.rand(mlp.num_inputs)\n",
        "  #Perform forward prop\n",
        "  result = mlp.forwrad_propagate(inputs)\n",
        "  #Print result\n",
        "  print(\"the network inputs is{}\".format(inputs))\n",
        "  print(\"the network output is{}\".format(result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF7yfTzprPlv"
      },
      "source": [
        "# Forward、Backward propagation and Gradient descent\n",
        "- Forward propagation：input --> NN Model -->predictions --> label --> loss\n",
        "- Backward propogation：caculate how each parameter(weight and bias) affect the loss and to optimize the parameter for minimizing the loss\n",
        "\n",
        "- Gradient Descent：Algrorithm to opitimize the weight of model for minimizing loss\n",
        "\n",
        "- article：[Back-propagation](https://medium.com/ai-academy-taiwan/back-propagation-3946e8ed8c55)、[Gradient Descent](https://medium.com/@gatorsquare/ml-gradient-descent-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-c664b5874e5c)、[機器/深度學習-基礎數學(二):梯度下降法(gradient descent)](https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E6%95%B8%E5%AD%B8-%E4%BA%8C-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-gradient-descent-406e1fd001f)、[一文看懂所有梯度下降优化算法理论](https://blog.csdn.net/comway_Li/article/details/81320787)、[Google ML - Lesson 7 - 梯度下降法 (Gradient Descent)介紹](https://ithelp.ithome.com.tw/articles/10218980)\n",
        "-Video：[A.I 人工智慧 - 機器學習 課程- 03 machine learning 梯度下降法 Gradient descent](https://www.youtube.com/watch?v=UW7zYzdm0ic&ab_channel=2018Mike)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEs6CDf514Re"
      },
      "source": [
        "# Demonstrate the process of MPL-buliding process with forward propagation,backward-propagation,gredient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeFBB_lt6UjR"
      },
      "source": [
        "#Build a simple MLP from scratch\n",
        "#whole process of MLP with backward propagation and gredien decent from scratch\n",
        "#Through this exercise, we should achieve:\n",
        " # save activations and deravitives\n",
        " # implement bakpropagation\n",
        " # implement gradient decent\n",
        " # implement train\n",
        " # train our net with some dummy dataset\n",
        " # make some predictions\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "from random import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class MLP:\n",
        "  def __init__(self,num_inputs = 3, num_layers = [3,5], num_outputs = 2):\n",
        "    self.num_inputs = num_inputs \n",
        "    self.num_layers = num_layers\n",
        "    self.num_outputs = num_outputs\n",
        "    self.weights = list()\n",
        "    self.activations = list()\n",
        "    self.deravitives = list()\n",
        "    layers = [self.num_inputs] + self.num_layers + [self.num_outputs]\n",
        "    #Create initial random weights matrix\n",
        "    for i in range(len(layers)-1):\n",
        "      w = np.random.rand(layers[i],layers[i+1])\n",
        "      self.weights.append(w)\n",
        "\n",
        "    for i in range(len(layers)):\n",
        "      a = np.zeros(layers[i])\n",
        "      self.activations.append(a)\n",
        "\n",
        "    for i in range(len(layers)-1): #Cause the number derivative matress is equal to the wieghtd matress\n",
        "      d = np.zeros(layers[i])\n",
        "      self.deravitives.append(i)\n",
        "\n",
        "  def sigmoid(self,x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "  def forward_propagation(self,inputs):\n",
        "    activations = inputs\n",
        "    self.activations[0] = inputs\n",
        "    for i,w in enumerate(self.weights):\n",
        "      #caculate the summation\n",
        "      net_inputs = np.dot(activations,w)\n",
        "      #caculate the activate function by appling net inputs to sigmoid function\n",
        "      activations = self.sigmoid(net_inputs)\n",
        "      self.activations[i+1] = activations\n",
        "\n",
        "    return activations\n",
        "\n",
        "  def backward_propagation(self,error,verbose=False):\n",
        "     for i in reversed(range(len(self.deravitives))):\n",
        "       activations = self.activations[i+1]\n",
        "       delta = error * self.sigmoid_derivative(activations)\n",
        "       delta_reshaped = delta.reshape(delta.shape[0],-1).T\n",
        "       current_activations = self.activations[i]\n",
        "       current_activations_reshaped = current_activations.reshape(current_activations.shape[0],-1)\n",
        "       self.deravitives[i] = np.dot(current_activations_reshaped,delta_reshaped)\n",
        "       error = np.dot(delta,self.weights[i].T)\n",
        "     return error\n",
        "\n",
        "  def sigmoid_derivative(self,x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "  def gradient_descent(self,learning_rate):\n",
        "    for i in range(len(self.weights)):\n",
        "      weights = self.weights[i]\n",
        "      # print('Original W{} {}'.format(i,weights))\n",
        "\n",
        "      deravitives = self.deravitives[i]\n",
        "\n",
        "      weights += deravitives * learning_rate\n",
        "      # print(\"Updated W{} {}\".format(i,weights))\n",
        "  \n",
        "  def train(self,inputs,targets,epochs,learning_rate):\n",
        "    for i in range(epochs):\n",
        "      sum_error = 0\n",
        "      for input,target in zip(inputs,targets):\n",
        "      \n",
        "        #forward propagation\n",
        "\n",
        "        output = mlp.forward_propagation(input)\n",
        "\n",
        "        #caculate the error\n",
        "        error = target - output\n",
        "\n",
        "        #backwar propagation\n",
        "        mlp.backward_propagation(error,verbose=True)\n",
        "\n",
        "        #Gredien descent\n",
        "        mlp.gradient_descent(learning_rate)\n",
        "\n",
        "      sum_error += self.mse(target,output)\n",
        "\n",
        "      print(\"Error: {} at epoch {}\".format(sum_error/len(inputs),i))\n",
        "\n",
        "\n",
        "  def mse(self,output,target):\n",
        "    return np.average((target - output) ** 2)\n",
        "#save actitvation and deravitive\n",
        "#\n",
        "#\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  #Create a dataset to train a network for sum operation\n",
        "  inputs = np.array([[random() / 2 for _ in range(2)] for _ in range(1000)])\n",
        "  targets = np.array([[i[0] + i[1]] for i in inputs])\n",
        "\n",
        "  #create an mlp\n",
        "  mlp = MLP(2,[5],1)\n",
        "\n",
        "  #train the MLP by train dataset\n",
        "  mlp.train(inputs,targets,50,1)\n",
        "  #create dummy data\n",
        "  input = np.array([0.1,0.3])\n",
        "  target = np.array([0.4])\n",
        "\n",
        "  #Predict the output by MLP:predict how many does 0.1 + 0.3 equal to?\n",
        "  output = mlp.forward_propagation(input)\n",
        "  print(\"Network believe that  {} + {} is equal to {}\".format(input[0],input[1],output[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR-4XTA-JJMR"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import numpy as np\n",
        "from random import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print(\"Found Gpu at: {}\".format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3P1crW9IbmI"
      },
      "source": [
        "\n",
        "\n",
        "#build a dataset\n",
        "#put in a module\n",
        "def generate_dataset(num_sample,test_size):\n",
        "  x = np.array([[random()/2 for _ in range(2)] for _ in range(num_sample)])\n",
        "  y = np.array([[i[0] + i[1]] for i in x])\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=test_size,random_state=1)\n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  if __name__ == \"__main__\":\n",
        "    x_train, x_test, y_train, y_test = generate_dataset(50000,0.2)\n",
        "    \n",
        "    # build a model: 2 -> 5 - > 1\n",
        "    model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(5, input_dim = 2, activation='sigmoid'),\n",
        "      tf.keras.layers.Dense(1, input_dim = 5, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    # compile model\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)#SGD: stochastic gredient descent\n",
        "    model.compile(optimizer = optimizer, loss=\"MSE\")\n",
        "    \n",
        "    # train model\n",
        "    model.fit(x_train,y_train,epochs=100,use_multiprocessing=True)\n",
        "\n",
        "    # evaluate model\n",
        "    print(\"\\nModel evaluate\")\n",
        "    model.evaluate(x_test,y_test,verbose=1)\n",
        "\n",
        "    # make predicts\n",
        "    data = np.array([[0.1,0.1],[0.2,0.2]])\n",
        "    predictions = model.predict(data)\n",
        "\n",
        "    print(\"\\nmake Preidiction:\")\n",
        "    for d,p in zip(data,predictions):\n",
        "      print(\"{} + {} = {}\".format(d[0],d[1],p[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGsVzcFvhqYS"
      },
      "source": [
        "# Procession audio data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_BO7o1S69X1"
      },
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "main_path = \"/content/drive/MyDrive/adventure_time/audio data\"\n",
        "os.chdir(main_path)\n",
        "os.listdir()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FARtxB-or9W"
      },
      "source": [
        "import librosa, librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "file = \"blues.00000.wav\"\n",
        "\n",
        "#load Waveform\n",
        "signal, sr = librosa.load(file,sr = 22050) #signal(1d array) contain values = sr * T -> 22050 *30\n",
        "\n",
        "#Visiualize Waveform data\n",
        "librosa.display.waveplot(signal, sr = sr)\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.title(\"Amplitude all over time\")\n",
        "plt.show()\n",
        "\n",
        "#Move waveform data from time domain to frequency domain by ffy\n",
        "#fft -> sepctrum\n",
        "fft = np.fft.fft(signal)\n",
        "magnitude = np.abs(fft) #get the magnitude of each frequency bin \n",
        "              # indicating the contricution of each bint to overall sound\n",
        "\n",
        "frequency = np.linspace(0,sr,len(magnitude))\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Magnitude')\n",
        "plt.title(\"Magnitude of each frequency bin\")\n",
        "# plt.plot(frequency,magnitude) #From the plot, we find most energy come from low frequency sound\n",
        "\n",
        "#By fft property, the plot is symmetrical,the point of symmetry is at half of freqauency label\n",
        "#Focus on half of samples\n",
        "left_magnitude = magnitude[:int(len(frequency)/2)]\n",
        "left_frequency = frequency[:int(len(frequency)/2)]\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Magnitude')\n",
        "plt.title(\"Magnitude of half frequency bin\")\n",
        "# plt.plot(left_frequency,left_magnitude)\n",
        "\n",
        "#stft -> spectrum\n",
        "num_sample_stft = 2048 #number of sample comtainning in a stft window\n",
        "hop_length = 512 #the  amount of shitting to right of each stft \n",
        "\n",
        "\n",
        "stft = librosa.core.stft(signal, hop_length =hop_length,n_fft=num_sample_stft)\n",
        "spectrum = np.abs(stft)\n",
        "\n",
        "log_spectrum = librosa.amplitude_to_db(spectrum)#convert the scale to logarithm\n",
        "                                                #to limit the range from 0-120 dB instead\n",
        "\n",
        "librosa.display.specshow(log_spectrum,sr=sr,hop_length=hop_length)\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Frequency')\n",
        "plt.colorbar()\n",
        "plt.title(\"Magnitude based on frequency and time domain\")\n",
        "plt.show()\n",
        "\n",
        "#MFCCs:frequency domain feature\n",
        "MFCCs = librosa.feature.mfcc(signal,n_fft = num_sample_stft,hop_length=hop_length,n_mfcc=13)\n",
        "librosa.display.specshow(MFCCs,sr=sr,hop_length=hop_length)\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('MFCCs')\n",
        "plt.title(\"Magnitude based on frequency(MFCCs) and time domain\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gyjbvHjRj8y"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import math\n",
        "import librosa\n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJQM2fgJIviY"
      },
      "source": [
        "#Rediraction\n",
        "os.chdir(\"/content/drive/MyDrive/adventure_time/audio data/Data/genres_original/\")\n",
        "main_path  = os.getcwd()\n",
        "for filepath,filedir,filename in os.walk(main_path):\n",
        "  \n",
        "  print(filepath)\n",
        "  # print(filepath.split(\"/\")[-1])\n",
        "  # print(filedir)\n",
        "  print(filename)\n",
        "  print(\"_\"*10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TGdBN7gLEj5"
      },
      "source": [
        "# Create Json file with MFCC form music data datase "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4STnHdcUiyL"
      },
      "source": [
        "json_path = \"data_10.json\"\n",
        "Sample_rate = 22050\n",
        "Duration = 30\n",
        "Sample_per_track = Sample_rate *Duration\n",
        "\n",
        "def save_mfcc(dataset_path, json_path,n_mfcc=13,nftt=2048,hop_length=512,num_segments=5):\n",
        "\n",
        "# Extract MFCCs from music dataset and save them into a json file \n",
        "# along with genres albels.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #dictionary to store data\n",
        "\n",
        "  data = {\n",
        "      \"mapping\":[],\n",
        "      \"mfcc\":[],\n",
        "      'labels':[]\n",
        "  }\n",
        "  # the number of data point in each segement\n",
        "  num_sample_per_segment = int(Sample_per_track / num_segments)\n",
        "  #To make sure the data point is eaqual in each mfcc vector in each  segment\n",
        "  expect_num_mfcc_vector_per_segment = math.ceil(num_sample_per_segment/hop_length)\n",
        "\n",
        "  #loop through all the genres\n",
        "  for i ,(dirpath,dirname,filename) in enumerate(os.walk(dataset_path)):\n",
        "    if dirpath is not dataset_path:\n",
        "      # print(i)\n",
        "      #Save the semantic label\n",
        "      dirpath_components = dirpath.split('/') #genres/blues => ['genres','blues']\n",
        "      semantic_label = dirpath_components[-1]\n",
        "      data['mapping'].append(semantic_label)\n",
        "      print(\"\\nProcessing {}\".format(semantic_label))\n",
        "      print(\"num of files in {} : {}\".format(semantic_label,len(filename)))\n",
        "      #Process files for a specific genres\n",
        "      for f in filename:\n",
        "\n",
        "        #Laod the audio file by librosa\n",
        "        # print(f)\n",
        "        file_path = os.path.join(dirpath,f)\n",
        "        signal ,sr = librosa.load(file_path,sr = Sample_rate,)\n",
        "\n",
        "        #We want analyze mafcc at segment level\n",
        "        #Process segments extracting mfcc and storing data\n",
        "\n",
        "        #\n",
        "        for s in range(num_segments):\n",
        "          start_sample = num_sample_per_segment *s\n",
        "          finall_sample = start_sample + num_sample_per_segment\n",
        "          mfcc = librosa.feature.mfcc(signal[start_sample:finall_sample],\n",
        "                                      sr = Sample_rate,\n",
        "                                      n_fft = nftt,\n",
        "                                      n_mfcc = n_mfcc,\n",
        "                                      hop_length = hop_length)\n",
        "          mfcc = mfcc.T\n",
        "\n",
        "          #Store mfcc for segment if it has the expected length\n",
        "          if len(mfcc) == expect_num_mfcc_vector_per_segment:\n",
        "            data['mfcc'].append(mfcc.tolist())\n",
        "            #\n",
        "            data['labels'].append(i -1)\n",
        "            print(\"{},segment :{}\".format(file_path,s+1))\n",
        "            print(\"_\"*10)\n",
        "      # print(\"num of segment in {} : {}\".format(semantic_label, num_segments))\n",
        "      # print(\"num of mfccs of each segment in {} : {}\".format(semantic_label, expect_num_mfcc_vector_per_segment))\n",
        "  \n",
        "  os.chdir(\"/content/drive/MyDrive/adventure_time/audio data/MFCCs data\")\n",
        "  print(\"length of mfcc vector : {}\".format(len(data['mfcc'])))\n",
        "  print(\"Start to write json data\")\n",
        "  #Save MFCCs to json file\n",
        "  with open(json_path, \"w\") as fp:\n",
        "    json.dump(data, fp,indent =4)\n",
        "  print(\"Mission Complete\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  save_mfcc(main_path,json_path,num_segments = 10)\n",
        "\n",
        "# files.download(\"data_10.json\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o315G4tqkXYH"
      },
      "source": [
        "\n",
        "main_path = \"/content/drive/MyDrive/adventure_time/audio data/MFCCs data\"\n",
        "os.chdir(main_path)\n",
        "# main_path  = os.getcwd()\n",
        "# main_path\n",
        "for filepath,filedir,filename in os.walk(main_path):\n",
        "  print(filepath)\n",
        "  # print(filepath.split(\"/\")[-1])\n",
        "  # print(filedir)\n",
        "  print(filename)\n",
        "  print(\"_\"*10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4cAakcelIyJ"
      },
      "source": [
        "# Build a music classification by DNN with Dropout, regularization to prevent overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ykWqDOWDgWB"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "dataset_path = \"data_10.json\"\n",
        "# load data\n",
        "#Create function to load data\n",
        "def load_data(dataset_path):\n",
        "  with open(dataset_path ,'r') as fp:\n",
        "    data = json.load(fp)\n",
        "\n",
        "\n",
        "  #Convert a list into np.array\n",
        "  input = np.array(data['mfcc'])\n",
        "  target = np.array(data['labels'])\n",
        "  return input,target\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  input, target = load_data(dataset_path)\n",
        "\n",
        "  #split the data into train and test set\n",
        "  input_train,input_test,target_train,target_test = train_test_split(input,target,test_size=0.3)\n",
        "\n",
        "\n",
        "  #Build network architecture\n",
        "  model = keras.Sequential([\n",
        "    #input layer\n",
        "    keras.layers.Flatten(input_shape =(input.shape[1],input.shape[2])),\n",
        "    #1st hidden layer,containning regularizer to add penalty to error function\n",
        "    keras.layers.Dense(512,activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "    #use drop out to solve overfitting\n",
        "    keras.layers.Dropout(0.1),\n",
        "    \n",
        "    #2nd hidden layer,containning regularizer to add penalty to error function\n",
        "    keras.layers.Dense(256,activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "    #use drop out to solve overfitting\n",
        "    keras.layers.Dropout(0.1),\n",
        "\n",
        "    #3rd hidden layer,containning regularizer to add penalty to error function\n",
        "    keras.layers.Dense(128,activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "    #use drop out to solve overfitting\n",
        "    keras.layers.Dropout(0.1),\n",
        "\n",
        "    #4rd hidden layer,containning regularizer to add penalty to error function\n",
        "    keras.layers.Dense(64,activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "    #use drop out to solve overfitting\n",
        "    keras.layers.Dropout(0.1),\n",
        "\n",
        "    #output layer                            \n",
        "    keras.layers.Dense(10,activation='softmax')\n",
        "  ])\n",
        "  #complie network\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  model.summary()\n",
        "  #train network\n",
        "  history = model.fit(input_train,target_train,\n",
        "            validation_data=(input_test,target_test),\n",
        "            epochs=70,\n",
        "            batch_size=32)\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FBvpUclx9m2"
      },
      "source": [
        "# Overfitting Overview\n",
        "- Initially, model provides incredible performance on the train set whereas not so well on the test. \n",
        "- This issue cuases by overfitting\n",
        "- model train so well that it could perform well on train set and hard to generalize. Once putting some new data into model, it'll have poorly performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snf35v3Jv9GE"
      },
      "source": [
        "## What is drop out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUCcs7T8MQjU"
      },
      "source": [
        "## What is regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOnkI2eXyHjz"
      },
      "source": [
        "def plot_history(history):\n",
        "  fig, ax = plt.subplots(figsize = (10,10),nrows =2)\n",
        "\n",
        "  #First, Create accuracy subplot\n",
        "  ax[0].plot(history.history['accuracy'],label = 'train accuracy')\n",
        "  ax[0].plot(history.history['val_accuracy'],label = 'test accuracy')\n",
        "  ax[0].set_ylabel('Accuracy')\n",
        "  ax[0].legend(loc = 'lower right')\n",
        "  ax[0].set_title('Accuray eval')\n",
        "  #Second, Create error(loss) subplot\n",
        "  ax[1].plot(history.history['loss'], label = 'train error')\n",
        "  ax[1].plot(history.history['val_loss'], label = 'test error')\n",
        "  ax[1].set_ylabel('Error')\n",
        "  ax[1].set_xlabel('Epoch')\n",
        "  ax[1].legend(loc = 'upper right')\n",
        "  ax[1].set_title('Error eval')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "plot_history(history)\n",
        "#Draw a plot comparing result of both model, overfiting and overfiting-free models "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB2GuBm1pvuq"
      },
      "source": [
        "# Build a music classification using CNN model\n",
        "- what batchnormalization\n",
        "- what CNN\n",
        "- how CNN work\n",
        "- how to apply CNN step by step\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ytqwhb2p2_o"
      },
      "source": [
        "data_path = 'data_10.json'\n",
        "main_path =\"/content/drive/MyDrive/adventure_time/audio data/MFCCs data\"\n",
        "#Define a function to load data\n",
        "def load_data(data_path):\n",
        "  if os.getcwd != main_path:\n",
        "    os.chdir(main_path)\n",
        "  with open(data_path,'r') as fp:\n",
        "     data = json.load(fp)\n",
        "\n",
        "  x = np.array(data['mfcc'])\n",
        "  y = np.array(data['labels'])\n",
        "  return x ,y \n",
        "\n",
        "# Define a function to separate train,validation and test set\n",
        "def prepare_dataset(test_size,validation_size):\n",
        "  x,y = load_data(data_path)\n",
        "  \n",
        "  x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = test_size)\n",
        "\n",
        "  x_train,x_validation,y_train,y_validation = train_test_split(x_train,y_train,test_size = validation_size)\n",
        "\n",
        "  # For CNN, Input is a 4D array(batch_size, height, width, depth) \n",
        "  x_train = x_train[...,np.newaxis]\n",
        "  x_validation = x_validation[...,np.newaxis]\n",
        "  x_test = x_test[...,np.newaxis]\n",
        "  \n",
        "  return x_train,x_validation,x_test,y_train,y_validation,y_test\n",
        "\n",
        "def build_model(input_shape):\n",
        "  #define a model instance\n",
        "  model = keras.Sequential()\n",
        "  #Create 1st convolution and max pooling layer with batch normaliztion\n",
        "  model.add(keras.layers.Conv2D(32,(3,3),activation='relu',input_shape = input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((3,3),strides=(2,2),padding='same'))\n",
        "\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "  #Create 2nd convolution and max pooling layer with batch normaliztion\n",
        "  model.add(keras.layers.Conv2D(32,(3,3),activation='relu',input_shape =input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((3,3),strides=(2,2),padding='same'))\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "  #Create 3rd convolution and max pooling layer with batch normaliztion\n",
        "  model.add(keras.layers.Conv2D(32,(2,2),activation='relu',input_shape = input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((2,2),strides=(2,2),padding='same'))\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "  #flatten the outputand feed it into dense layer\n",
        "  model.add(keras.layers.Flatten())\n",
        "  model.add(keras.layers.Dense(64,activation='relu',kernel_regularizer=keras.regularizers.l2(0.01)))\n",
        "  model.add(keras.layers.Dropout(0.3))\n",
        "  #Output\n",
        "  model.add(keras.layers.Dense(10,activation='softmax'))\n",
        "\n",
        "  return model\n",
        "\n",
        "def predict(model,x,y):\n",
        "  genres = data['mapping']\n",
        "  x = x[np.newaxis,...]\n",
        "  predictions = model.predict(x)\n",
        "  predict_index = int(np.argmax(predictions,axis=1))\n",
        "  print(\"Expexted index : {}, predicted index : {}\".format(genres[y],genres[predict_index]))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  #Load data\n",
        "  preparing_data(data_path)\n",
        "\n",
        "  #Build a model\n",
        "  x_train,x_validation,x_test,y_train,y_validation,y_test = prepare_dataset(test_size=0.2,validation_size=0.3)\n",
        "  input_shape = (x_train.shape[1],x_train.shape[2],x_train.shape[3])\n",
        "  model = build_model(input_shape)\n",
        "  #Compile the metwork\n",
        "  optimizers = keras.optimizers.Adam(learning_rate=0.001)\n",
        "  model.compile(optimizer=optimizers,\n",
        "                loss=\"sparse_categorical_crossentropy\",\n",
        "                metrics = ['accuracy'])\n",
        "  #Train the CNN model\n",
        "  history = model.fit(x_train,y_train,batch_size=32,epochs=40,\n",
        "             validation_data=(x_validation,y_validation),\n",
        "             #use multi process\n",
        "             use_multiprocessing=True)\n",
        "  #Evaluate the CNN on the test set\n",
        "  test_error, test_accuracy = model.evaluate(x_test,y_test)\n",
        "  print(\"Accuracy to the test set is {}\".format(test_accuracy))\n",
        "  #make a predict\n",
        "  with open(data_path,'r') as fp:\n",
        "    data = json.load(fp)\n",
        "  X = x_test[100]\n",
        "  Y = y_test[100]\n",
        "  predict(model,X,Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7ISOAozIEF_"
      },
      "source": [
        "# Build a music classifiacation using RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QNC2WKHINv1"
      },
      "source": [
        "#Define datapath\n",
        "data_path = \"data_10.json\"\n",
        "main_path =\"/content/drive/MyDrive/adventure_time/audio data/MFCCs data\"\n",
        "#Define functions:\n",
        "#data load\n",
        "def load_data(data_path):\n",
        "  if os.getcwd != main_path:\n",
        "    os.chdir(main_path)\n",
        "  with open(data_path,'r') as fp:\n",
        "    data = json.load(fp)\n",
        "  \n",
        "  #Separate dictionary into two part data\n",
        "  x = np.array(data['mfcc'])\n",
        "  y = np.array(data['labels'])\n",
        "  return data,x,y\n",
        "#Separate data into train, test dataset\n",
        "\n",
        "def preparing_data(test_size,validation_size):\n",
        "  data,x,y = load_data(data_path)\n",
        "  x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=test_size)\n",
        "  x_train,x_validation,y_train,y_validation = train_test_split(x_train,y_train,test_size=validation_size)\n",
        "\n",
        "  return data,x_train,x_test,y_train,y_test,x_validation,y_validation\n",
        "#Built a modle pipeline\n",
        "def build_model(data_sahpe):\n",
        "  #data_shape:input data shape \n",
        "  #(number of MFCC, step length of each segement) -> (130,13)\n",
        "  #define a model instance\n",
        "  model = keras.Sequential()\n",
        "  #add layer for RNN:\n",
        "  #2 layers of LSTM -> sequence-to-sequence layer and pass-to-next-unit layer\n",
        "  model.add(keras.layers.LSTM(units=64,input_shape=data_shape,return_sequences=True))\n",
        "  model.add(keras.layers.LSTM(units=64))\n",
        "\n",
        "  #dense layer\n",
        "  model.add(keras.layers.Dense(64,activation='relu',kernel_regularizer=keras.regularizers.l2(0.01)))\n",
        "  #dropout layer\n",
        "  model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "  #output layer\n",
        "  model.add(keras.layers.Dense(10,activation='softmax'))\n",
        "\n",
        "  return model\n",
        "\n",
        "#Predict\n",
        "def prediction(model,x,y):\n",
        "  genres = data['mapping']\n",
        "\n",
        "  predictions = model.predict(x)\n",
        "  predict_index = int(np.argmax(predictions,axis=1))\n",
        "  print(\"Expexted index : {}, predicted index : {}\".format(genres[y],genres[predict_index]))\n",
        "  # pred = model.predict(x)\n",
        "  # predict_index = int(np.argmax(predictions,axis=1))\n",
        "  # print(\"the initial genres is:{}; The prediction is:{}\").format(y,pred)\n",
        "#Apply function to build model and make prediction\n",
        "if __name__ == '__main__':\n",
        "  #Load data\n",
        "  #Separate data into trainning set and testing set\n",
        "  data,x_train,x_test,y_train,y_test,x_validation,y_validation = preparing_data(0.3,0.3)\n",
        "\n",
        "  #Build a RNN model\n",
        "  #1.Get the data shape for modle\n",
        "  #2.Create neural network containning specific layer\n",
        "  #Compile model with optimizer, loss function, and validation metric\n",
        "  data_shape = (x_train.shape[1],x_train.shape[2])\n",
        "  model = build_model(data_shape)\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "  model.compile(optimizer=optimizer,\n",
        "         loss=\"sparse_categorical_crossentropy\",\n",
        "         metrics=['accuracy'])\n",
        "  history = model.fit(x_train,y_train,\n",
        "             batch_size=32,epochs=40,\n",
        "             validation_data=(x_validation,y_validation),\n",
        "             #use multi process\n",
        "             use_multiprocessing=True)\n",
        "  \n",
        "\n",
        "  #Evaluate the RNN on the test set\n",
        "  test_error, test_accuracy = model.evaluate(x_test,y_test)\n",
        "  print(\"Accuracy to the test set is {}\".format(test_accuracy))\n",
        "  #make a predict\n",
        "\n",
        "  X = x_test[100].reshape(1,130,13)\n",
        "  Y = y_test[100]\n",
        "  prediction(model,X,Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9WZK1lLQhmq"
      },
      "source": [
        "https://stackoverflow.com/questions/65327942/lstm-input-shape-error-input-0-is-incompatible-with-layer-sequential-1\n",
        "\n",
        "https://www.youtube.com/watch?v=Msk-A99oo10&ab_channel=TheSemicolon\n",
        "\n",
        "https://machinelearningmastery.com/make-predictions-long-short-term-memory-models-keras/\n",
        "\n",
        "https://stats.stackexchange.com/questions/326065/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other\n",
        "\n",
        "https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb3BGMaiP1ZT"
      },
      "source": [
        "from tensorflow.keras import layers, Model, utils\n",
        "utils.plot_model(model, show_layer_names=False, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGPSAVpG-y0x"
      },
      "source": [
        "X = x_test[100].reshape(1,130,13)\n",
        "Y = y_test[100]\n",
        "prediction(model,X,Y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8_TM9gt0hw1"
      },
      "source": [
        "def prediction(model,x,y):\n",
        "  genres = data['mapping']\n",
        "\n",
        "  predictions = model.predict(x)\n",
        "  predict_index = int(np.argmax(predictions,axis=1))\n",
        "  print(\"Expexted index : {}, predicted index : {}\".format(genres[y],genres[predict_index]))\n",
        "# model.predict(X)\n",
        "# data_shape\n",
        "X = x_test[100]\n",
        "Y = y_test[100]\n",
        "# prediction(model,X,Y)\n",
        "model.predict(X)\n",
        "genres = data['mapping']\n",
        "genres"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08lWFojiARp3"
      },
      "source": [
        "def plot_history(history):\n",
        "  fig, ax = plt.subplots(figsize = (10,10),nrows =2)\n",
        "\n",
        "  #First, Create accuracy subplot\n",
        "  ax[0].plot(history.history['accuracy'],label = 'train accuracy')\n",
        "  ax[0].plot(history.history['val_accuracy'],label = 'test accuracy')\n",
        "  ax[0].set_ylabel('Accuracy')\n",
        "  ax[0].legend(loc = 'lower right')\n",
        "  ax[0].set_title('Accuray eval')\n",
        "  #Second, Create error(loss) subplot\n",
        "  ax[1].plot(history.history['loss'], label = 'train error')\n",
        "  ax[1].plot(history.history['val_loss'], label = 'test error')\n",
        "  ax[1].set_ylabel('Error')\n",
        "  ax[1].set_xlabel('Epoch')\n",
        "  ax[1].legend(loc = 'upper right')\n",
        "  ax[1].set_title('Error eval')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYtW1j76yJIN"
      },
      "source": [
        "## Solving Overfitting:\n",
        "- sovle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc1ZRpji2ET-"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lA251SL8Ak6w"
      },
      "source": [
        "# Python function study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzeKDyZoQrif"
      },
      "source": [
        "## os.walk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2B2ZbHB8hUz"
      },
      "source": [
        "os.chdir(\"/content/drive/MyDrive/adventure_time/audio data/Data/genres_original\")\n",
        "os.listdir()\n",
        "main_path = os.getcwd()\n",
        "\n",
        "for dirpath, dirnames, filenames in os.walk(main_path,topdown=True):\n",
        "  if dirpath is not main_path:\n",
        "    print(dirpath)\n",
        "    print(dirnames)\n",
        "\n",
        "    print(\"-\" *)\n",
        "    # print(filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKXNwbc369Gr"
      },
      "source": [
        "# Model knowledge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bYaoO8cI9t4"
      },
      "source": [
        "## What is epoch definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jNJK67pJP2i"
      },
      "source": [
        "- epoch is the process that use all batchs to update weight.\n",
        "\n",
        "- 1 epoch = do the gredient descent allover batchs 1 times\n",
        "\n",
        "- how many times model updating weight depends on  num of epoch * num of batchs\n",
        "\n",
        "- [more info about epoch](https://wenwu53.com/keras2-0/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2GU6vnRJO8a"
      },
      "source": [
        "## What is batch\n",
        "- batch means the sub-dataset\n",
        "- batch size means the datapoint in the sub-dataset\n",
        "- total size = num of batch * batch size\n",
        "- [more info about batch](https://wenwu53.com/keras2-0/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIpFLY3s52v_"
      },
      "source": [
        "## Type of batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uEbQc5y59CL"
      },
      "source": [
        "1. Stochastic\n",
        "  \n",
        "  - Caculate gradient on 1 sample\n",
        "  - Quick, but inaccurate\n",
        "\n",
        "2. Full Batch\n",
        "  - Compute gradient on the whole trainning set\n",
        "  - Slow, memroy intensive, accurate\n",
        "\n",
        "3. Mini Batch\n",
        "  - Compute gradient on a subset of data set(around 16 - 128 Sample)\n",
        "  - more suitable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zSzz1mOqCLu"
      },
      "source": [
        "MFCC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfY2Y2kbhpdL"
      },
      "source": [
        "layers = [3] + [3,5] + [2]\n",
        "weights = list()\n",
        "for i in range(len(layers)-1):\n",
        "  w = np.random.rand(layers[i],layers[i+1])\n",
        "  weights.append(w)\n",
        "\n",
        "\n",
        "print(weights)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8pEzx-QpM56"
      },
      "source": [
        "# [i  for i in range(len(layers)-1)]\n",
        "np.random.rand(layers[0],layers[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEpVUbxkDPIn"
      },
      "source": [
        "outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwPHytoJlHw3"
      },
      "source": [
        "np.array[0.2,0.1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmFHxRceYd_O"
      },
      "source": [
        "_import numpy as np\n",
        "import tensorflow\n",
        "from random import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def  generate_dataset(num_sample,test_size):\n",
        "  x = \n",
        "  y ="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXz6txqsBYh6"
      },
      "source": [
        "np.zeros(2,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjmpNhwciD4C"
      },
      "source": [
        "# What is loss function\n"
      ]
    }
  ]
}